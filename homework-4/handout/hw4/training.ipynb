{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "BZ9rFdlHHKwk"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tests import test_prediction, test_generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "8cRGLoGqHLYW"
      },
      "outputs": [],
      "source": [
        "# load all that we need\n",
        "\n",
        "dataset = np.load('../dataset/wiki.train.npy', allow_pickle=True)\n",
        "devset = np.load('../dataset/wiki.valid.npy', allow_pickle=True)\n",
        "fixtures_pred = np.load('../fixtures/prediction.npz')  # dev\n",
        "fixtures_gen = np.load('../fixtures/generation.npy')  # dev\n",
        "fixtures_pred_test = np.load('../fixtures/prediction_test.npz')  # test\n",
        "fixtures_gen_test = np.load('../fixtures/generation_test.npy')  # test\n",
        "vocab = np.load('../dataset/vocab.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "P5XxkKabK9GN"
      },
      "outputs": [],
      "source": [
        "# set device as per system\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q97xXlHUHQj3",
        "outputId": "b569b417-9860-42b7-eddc-009d9a3fa911"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<generator object LanguageModelDataLoader.__iter__ at 0x7f79c86de050>"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# data loader\n",
        "\n",
        "\n",
        "class LanguageModelDataLoader(DataLoader):\n",
        "    \"\"\"\n",
        "        TODO: Define data loader logic here\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, batch_size, shuffle=True):\n",
        "        self.dataset, self.batch_size, self.shuffle = dataset, batch_size, shuffle \n",
        "\n",
        "    def __iter__(self):\n",
        "        # concatenate dataset articles into single string\n",
        "        # dataset shape: (579,), dataset[0].shape: (3803,)\n",
        "        concatenate_string = np.concatenate(self.dataset) # concatenated shape: (2075677,)\n",
        "        \n",
        "        # generate input, output sequences eg: I ate an apple -> inp_seq: I ate an, out_seq: ate an apple\n",
        "        # (also convert to torch tensors)\n",
        "        input_sequence = torch.as_tensor(concatenate_string[:-1]) # first element to second last element \n",
        "        output_sequence = torch.as_tensor(concatenate_string[1:]) # second element to last element\n",
        "\n",
        "        # calculate excess length while batching and truncate it off\n",
        "        excess_length = len(input_sequence)%self.batch_size\n",
        "        truncated_length = len(input_sequence) - excess_length\n",
        "        input_sequence, output_sequence = input_sequence[:truncated_length], output_sequence[:truncated_length]\n",
        "\n",
        "        # convert to long tensors\n",
        "        input_sequence, output_sequence = (input_sequence).type(torch.LongTensor), (output_sequence).type(torch.LongTensor)\n",
        "\n",
        "\n",
        "        # batch the input and output sequences\n",
        "        num_batches = truncated_length // self.batch_size\n",
        "        input_sequence = input_sequence.reshape(self.batch_size, num_batches)\n",
        "        output_sequence = output_sequence.reshape(self.batch_size, num_batches)\n",
        "        # print(f'input sequence: {input_sequence.shape} \\noutput sequence: {output_sequence.shape}')\n",
        "\n",
        "        # YIELD single batch of input, output for each batch (since we are designing an iter)\n",
        "        prev = curr = 0 # current index for indexing data from sequences\n",
        "        while curr < num_batches:\n",
        "            # random BPTT length, https://arxiv.org/pdf/1708.02182.pdf section 5\n",
        "            bptt_length = self.random_length()\n",
        "            prev = curr\n",
        "            curr += bptt_length\n",
        "            yield input_sequence[:, prev:curr], output_sequence[:, prev:curr]\n",
        "\n",
        "\n",
        "    # random BPTT length, https://arxiv.org/pdf/1708.02182.pdf section 5\n",
        "    def random_length(self):\n",
        "        random_probability = np.random.random_sample()\n",
        "        if random_probability > 0.95:\n",
        "            bptt_length = np.random.normal(70, 5)\n",
        "        else:\n",
        "            bptt_length = np.random.normal(35, 5)\n",
        "        return round(bptt_length) #round off so we have integers\n",
        "\n",
        "\n",
        "        \n",
        "# test code\n",
        "loader = LanguageModelDataLoader(dataset=dataset, batch_size=60, shuffle=True)\n",
        "loader.__iter__()\n",
        "# print(f'x:{x.shape}, y:{y.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSvPe4aUHRyu",
        "outputId": "792169e3-0eea-4ca1-f8c5-8e81ba52d9b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LanguageModel(\n",
            "  (embedding): Embedding(33278, 400)\n",
            "  (lstm): LSTM(400, 1150, num_layers=3, batch_first=True)\n",
            "  (linear): Linear(in_features=1150, out_features=33278, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# model\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "        TODO: Define your model here\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        # embedding size = 400 (https://arxiv.org/pdf/1708.02182.pdf section 5)\n",
        "        self.embedding = nn.Embedding(num_embeddings = vocab_size, embedding_dim = 400) # simple lookup table that stores embeddings of a fixed dictionary and size\n",
        "        # hidden size = 1150 (https://arxiv.org/pdf/1708.02182.pdf section 5)\n",
        "        self.lstm = nn.LSTM(input_size=400, hidden_size=1150, num_layers=3, batch_first=True)\n",
        "        # linear output = vocabulary size\n",
        "        self.linear = nn.Linear(in_features=1150, out_features=vocab_size)\n",
        "\n",
        "    def forward(self, x, hiddens=None):\n",
        "        # Feel free to add extra arguments to forward (like an argument to pass in the hiddens)\n",
        "        # embedding\n",
        "        embeddings = self.embedding(x) \n",
        "        # lstm / rnn\n",
        "        out, hiddens = self.lstm(embeddings, hiddens) if hiddens else self.lstm(embeddings) #operate on hidden states only if they are available\n",
        "        # linear\n",
        "        out = self.linear(out) \n",
        "        return out, hiddens\n",
        "\n",
        "model = LanguageModel(len(vocab))\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "2gs1TOJ8SJfa"
      },
      "outputs": [],
      "source": [
        "# model hyperparameters\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "0GGpyogEHdgD"
      },
      "outputs": [],
      "source": [
        "# model trainer\n",
        "import time \n",
        "\n",
        "class LanguageModelTrainer:\n",
        "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
        "        \"\"\"\n",
        "            Use this class to train your model\n",
        "        \"\"\"\n",
        "        # feel free to add any other parameters here\n",
        "        self.model = model\n",
        "        self.loader = loader\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.predictions = []\n",
        "        self.predictions_test = []\n",
        "        self.generated_logits = []\n",
        "        self.generated = []\n",
        "        self.generated_logits_test = []\n",
        "        self.generated_test = []\n",
        "        self.epochs = 0\n",
        "        self.max_epochs = max_epochs\n",
        "        self.run_id = run_id\n",
        "        \n",
        "        # TODO: Define your optimizer and criterion here\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY) #\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train() # set to training mode\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        start_time = time.time()\n",
        "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
        "            if batch_num % 50 == 0:\n",
        "                print(f'batch : {batch_num} \\ttotal time elapsed : {round(time.time()-start_time, 2)} sec')\n",
        "            epoch_loss += self.train_batch(inputs, targets)\n",
        "        epoch_loss = epoch_loss / (batch_num + 1)\n",
        "        self.epochs += 1\n",
        "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, epoch_loss))\n",
        "        print(f'time taken = {round((time.time()-start_time)/60, 2)} min')\n",
        "        self.train_losses.append(epoch_loss)\n",
        "\n",
        "    def train_batch(self, inputs, targets):\n",
        "        \"\"\" \n",
        "            TODO: Define code for training a single batch of inputs\n",
        "        \n",
        "        \"\"\"\n",
        "        # initialize and move to the active device (make sure everything (inputs, model, outputs, targets) on same device)\n",
        "        self.optimizer.zero_grad()\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # get output from model\n",
        "        outputs, _ = self.model(inputs)\n",
        "\n",
        "        # reshape outputs and targets\n",
        "        outputs = outputs.reshape(-1, outputs.shape[2])\n",
        "        targets = targets.reshape(-1)\n",
        "        \n",
        "        # judge quality of output against the target using loss function\n",
        "        loss = self.criterion(outputs, targets)\n",
        "        \n",
        "        # optimize weights\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    \n",
        "    def test(self):\n",
        "        # don't change these\n",
        "        self.model.eval() # set to eval mode\n",
        "        predictions = TestLanguageModel.prediction(fixtures_pred['inp'], self.model) # get predictions\n",
        "        self.predictions.append(predictions)\n",
        "        generated_logits = TestLanguageModel.generation(fixtures_gen, 10, self.model) # generated predictions for 10 words\n",
        "        generated_logits_test = TestLanguageModel.generation(fixtures_gen_test, 10, self.model)\n",
        "        nll = test_prediction(predictions, fixtures_pred['out'])\n",
        "        generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
        "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
        "        self.val_losses.append(nll)\n",
        "        \n",
        "        self.generated.append(generated)\n",
        "        self.generated_test.append(generated_test)\n",
        "        self.generated_logits.append(generated_logits)\n",
        "        self.generated_logits_test.append(generated_logits_test)\n",
        "        \n",
        "        # generate predictions for test data\n",
        "        predictions_test = TestLanguageModel.prediction(fixtures_pred_test['inp'], self.model) # get predictions\n",
        "        self.predictions_test.append(predictions_test)\n",
        "            \n",
        "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, nll))\n",
        "        print('='*60)\n",
        "        return nll\n",
        "\n",
        "    def save(self):\n",
        "        # don't change these\n",
        "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
        "        torch.save({'state_dict': self.model.state_dict()},\n",
        "            model_path)\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated_test[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "dmyEd8K9Hgkx"
      },
      "outputs": [],
      "source": [
        "class TestLanguageModel:\n",
        "    def prediction(inp, model):\n",
        "        \"\"\"\n",
        "            TODO: write prediction code here\n",
        "            \n",
        "            :param inp:\n",
        "            :return: a np.ndarray of logits\n",
        "        \"\"\"\n",
        "        # every input across notebook needs to be converted to long tensor\n",
        "        # convert inputs to long tensor\n",
        "        inp = torch.LongTensor(inp)\n",
        "        # move to active device \n",
        "        inp = inp.to(device)\n",
        "        # get model output\n",
        "        out, out_lengths = model(inp)\n",
        "        out = out[:, -1]\n",
        "\n",
        "        # detatch logits array from tensor\n",
        "        predictions = out.cpu().detach().numpy()\n",
        "        return predictions\n",
        "\n",
        "        \n",
        "    def generation(inp, forward, model):\n",
        "        \"\"\"\n",
        "            TODO: write generation code here\n",
        "\n",
        "            Generate a sequence of words given a starting sequence.\n",
        "            :param inp: Initial sequence of words (batch size, length)\n",
        "            :param forward: number of additional words to generate\n",
        "            :return: generated words (batch size, forward)\n",
        "        \"\"\"         \n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            result = [] # array of strings of length = forward\n",
        "            # change to long type\n",
        "            inp = torch.LongTensor(inp)\n",
        "            # move inputs to device\n",
        "            inp = inp.to(device)\n",
        "            hidden = None\n",
        "            for i in range(forward):\n",
        "                out, hidden = model(inp, hidden) if hidden else model(inp) # pass in hidden input only if available\n",
        "                predictions = torch.argmax(out, dim=2) \n",
        "                predictions = predictions[:,-1]\n",
        "                inp = predictions.unsqueeze(1)\n",
        "                result.append(inp)\n",
        "        # concatenate result shape\n",
        "        result = torch.cat(result, dim=1)\n",
        "\n",
        "        # detatch words array from tensor\n",
        "        result = result.cpu().detach().numpy()        \n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "mT8MrAVtHi2E"
      },
      "outputs": [],
      "source": [
        "# TODO: define other hyperparameters here\n",
        "\n",
        "NUM_EPOCHS = 6 # based on writeup\n",
        "BATCH_SIZE = 80 # based on https://arxiv.org/pdf/1708.02182.pdf section 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAvo1G2BHkyA",
        "outputId": "d98e63c0-7d02-416e-ebd5-01d17bf4938a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving models, predictions, and generated words to ./experiments/1639367590\n"
          ]
        }
      ],
      "source": [
        "run_id = str(int(time.time()))\n",
        "if not os.path.exists('./experiments'):\n",
        "    os.mkdir('./experiments')\n",
        "os.mkdir('./experiments/%s' % run_id)\n",
        "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "VKFNviGiHp3-"
      },
      "outputs": [],
      "source": [
        "loader = LanguageModelDataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "model = LanguageModel(len(vocab))\n",
        "model = model.to(device)\n",
        "trainer = LanguageModelTrainer(model=model, loader=loader, max_epochs=NUM_EPOCHS, run_id=run_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfyIBZZjVwRX",
        "outputId": "bf9f77e3-7c32-40f7-ec72-06ab6964a02a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataloader = 579\n"
          ]
        }
      ],
      "source": [
        "print(f'length of dataloader = {len(loader.dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgBADfNZHqfq",
        "outputId": "479330fb-630a-432b-951b-cfd7bd478585"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch : 0 \ttotal time elapsed : 0.01 sec\n",
            "batch : 50 \ttotal time elapsed : 36.66 sec\n",
            "batch : 100 \ttotal time elapsed : 72.51 sec\n",
            "batch : 150 \ttotal time elapsed : 110.1 sec\n",
            "batch : 200 \ttotal time elapsed : 148.35 sec\n",
            "batch : 250 \ttotal time elapsed : 185.83 sec\n",
            "batch : 300 \ttotal time elapsed : 223.14 sec\n",
            "batch : 350 \ttotal time elapsed : 260.06 sec\n",
            "batch : 400 \ttotal time elapsed : 297.58 sec\n",
            "batch : 450 \ttotal time elapsed : 336.45 sec\n",
            "batch : 500 \ttotal time elapsed : 374.18 sec\n",
            "batch : 550 \ttotal time elapsed : 411.69 sec\n",
            "batch : 600 \ttotal time elapsed : 449.06 sec\n",
            "batch : 650 \ttotal time elapsed : 487.18 sec\n",
            "batch : 700 \ttotal time elapsed : 522.77 sec\n",
            "[TRAIN]  Epoch [2/6]   Loss: 7.0571\n",
            "time taken = 8.82 min\n",
            "[VAL]  Epoch [2/6]   Loss: 6.0492\n",
            "============================================================\n",
            "Saving model, predictions and generated output for epoch 0 with NLL: 6.0492325\n",
            "batch : 0 \ttotal time elapsed : 0.01 sec\n",
            "batch : 50 \ttotal time elapsed : 35.89 sec\n",
            "batch : 100 \ttotal time elapsed : 71.38 sec\n",
            "batch : 150 \ttotal time elapsed : 110.11 sec\n",
            "batch : 200 \ttotal time elapsed : 147.03 sec\n",
            "batch : 250 \ttotal time elapsed : 183.21 sec\n",
            "batch : 300 \ttotal time elapsed : 222.18 sec\n",
            "batch : 350 \ttotal time elapsed : 258.09 sec\n",
            "batch : 400 \ttotal time elapsed : 296.98 sec\n",
            "batch : 450 \ttotal time elapsed : 332.88 sec\n",
            "batch : 500 \ttotal time elapsed : 371.07 sec\n",
            "batch : 550 \ttotal time elapsed : 411.36 sec\n",
            "batch : 600 \ttotal time elapsed : 447.67 sec\n",
            "batch : 650 \ttotal time elapsed : 484.13 sec\n",
            "batch : 700 \ttotal time elapsed : 522.34 sec\n",
            "[TRAIN]  Epoch [3/6]   Loss: 6.0887\n",
            "time taken = 8.89 min\n",
            "[VAL]  Epoch [3/6]   Loss: 5.3978\n",
            "============================================================\n",
            "Saving model, predictions and generated output for epoch 1 with NLL: 5.3978324\n",
            "batch : 0 \ttotal time elapsed : 0.01 sec\n",
            "batch : 50 \ttotal time elapsed : 35.69 sec\n",
            "batch : 100 \ttotal time elapsed : 75.32 sec\n",
            "batch : 150 \ttotal time elapsed : 112.77 sec\n",
            "batch : 200 \ttotal time elapsed : 153.2 sec\n",
            "batch : 250 \ttotal time elapsed : 189.04 sec\n",
            "batch : 300 \ttotal time elapsed : 227.48 sec\n",
            "batch : 350 \ttotal time elapsed : 265.19 sec\n",
            "batch : 400 \ttotal time elapsed : 302.24 sec\n",
            "batch : 450 \ttotal time elapsed : 339.02 sec\n",
            "batch : 500 \ttotal time elapsed : 375.52 sec\n",
            "batch : 550 \ttotal time elapsed : 413.91 sec\n",
            "batch : 600 \ttotal time elapsed : 452.38 sec\n",
            "batch : 650 \ttotal time elapsed : 490.48 sec\n",
            "batch : 700 \ttotal time elapsed : 527.54 sec\n",
            "[TRAIN]  Epoch [4/6]   Loss: 5.6062\n",
            "time taken = 8.91 min\n",
            "[VAL]  Epoch [4/6]   Loss: 5.1104\n",
            "============================================================\n",
            "Saving model, predictions and generated output for epoch 2 with NLL: 5.1104355\n",
            "batch : 0 \ttotal time elapsed : 0.01 sec\n",
            "batch : 50 \ttotal time elapsed : 37.61 sec\n",
            "batch : 100 \ttotal time elapsed : 74.1 sec\n",
            "batch : 150 \ttotal time elapsed : 110.2 sec\n",
            "batch : 200 \ttotal time elapsed : 146.28 sec\n",
            "batch : 250 \ttotal time elapsed : 185.97 sec\n",
            "batch : 300 \ttotal time elapsed : 225.17 sec\n",
            "batch : 350 \ttotal time elapsed : 261.75 sec\n",
            "batch : 400 \ttotal time elapsed : 299.92 sec\n",
            "batch : 450 \ttotal time elapsed : 337.8 sec\n",
            "batch : 500 \ttotal time elapsed : 375.47 sec\n",
            "batch : 550 \ttotal time elapsed : 411.98 sec\n",
            "batch : 600 \ttotal time elapsed : 449.38 sec\n",
            "batch : 650 \ttotal time elapsed : 487.5 sec\n",
            "batch : 700 \ttotal time elapsed : 524.16 sec\n",
            "[TRAIN]  Epoch [5/6]   Loss: 5.3117\n",
            "time taken = 8.91 min\n",
            "[VAL]  Epoch [5/6]   Loss: 4.9230\n",
            "============================================================\n",
            "Saving model, predictions and generated output for epoch 3 with NLL: 4.9229994\n",
            "batch : 0 \ttotal time elapsed : 0.02 sec\n",
            "batch : 50 \ttotal time elapsed : 35.93 sec\n",
            "batch : 100 \ttotal time elapsed : 74.17 sec\n",
            "batch : 150 \ttotal time elapsed : 112.52 sec\n",
            "batch : 200 \ttotal time elapsed : 150.6 sec\n",
            "batch : 250 \ttotal time elapsed : 187.91 sec\n",
            "batch : 300 \ttotal time elapsed : 224.61 sec\n",
            "batch : 350 \ttotal time elapsed : 261.76 sec\n",
            "batch : 400 \ttotal time elapsed : 298.24 sec\n",
            "batch : 450 \ttotal time elapsed : 336.18 sec\n",
            "batch : 500 \ttotal time elapsed : 376.71 sec\n",
            "batch : 550 \ttotal time elapsed : 413.5 sec\n",
            "batch : 600 \ttotal time elapsed : 452.56 sec\n",
            "batch : 650 \ttotal time elapsed : 490.82 sec\n",
            "batch : 700 \ttotal time elapsed : 528.43 sec\n",
            "[TRAIN]  Epoch [6/6]   Loss: 5.0796\n",
            "time taken = 8.91 min\n",
            "[VAL]  Epoch [6/6]   Loss: 4.8226\n",
            "============================================================\n",
            "Saving model, predictions and generated output for epoch 4 with NLL: 4.8226085\n",
            "batch : 0 \ttotal time elapsed : 0.01 sec\n",
            "batch : 50 \ttotal time elapsed : 36.94 sec\n",
            "batch : 100 \ttotal time elapsed : 76.4 sec\n",
            "batch : 150 \ttotal time elapsed : 115.09 sec\n",
            "batch : 200 \ttotal time elapsed : 151.03 sec\n",
            "batch : 250 \ttotal time elapsed : 186.18 sec\n",
            "batch : 300 \ttotal time elapsed : 223.17 sec\n",
            "batch : 350 \ttotal time elapsed : 260.28 sec\n",
            "batch : 400 \ttotal time elapsed : 298.29 sec\n",
            "batch : 450 \ttotal time elapsed : 337.95 sec\n",
            "batch : 500 \ttotal time elapsed : 372.83 sec\n",
            "batch : 550 \ttotal time elapsed : 410.3 sec\n",
            "batch : 600 \ttotal time elapsed : 448.71 sec\n",
            "batch : 650 \ttotal time elapsed : 486.15 sec\n",
            "batch : 700 \ttotal time elapsed : 524.1 sec\n",
            "[TRAIN]  Epoch [7/6]   Loss: 4.8873\n",
            "time taken = 8.99 min\n",
            "[VAL]  Epoch [7/6]   Loss: 4.7296\n",
            "============================================================\n",
            "Saving model, predictions and generated output for epoch 5 with NLL: 4.7295513\n"
          ]
        }
      ],
      "source": [
        "best_nll = 1e30\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    trainer.train()\n",
        "    nll = trainer.test()\n",
        "    if nll < best_nll:\n",
        "        best_nll = nll\n",
        "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
        "        trainer.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "ZmkKZPMOI7HP",
        "outputId": "d6391f31-21e3-4685-93c3-a70f32c93a59"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Don't change these\n",
        "# plot training curves\n",
        "plt.figure()\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('NLL')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLj7xl9nI-m8",
        "outputId": "e7ac11e4-f100-4890-a6d3-303aa5d94166"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input | Output #0: while the group was en route , but only three were ultimately able to attack . None of them were | not damaged , and the ship was completed . <eol>\n",
            "Input | Output #1: <unk> , where he remained on loan until 30 June 2010 . <eol> = = = Return to Manchester United | = = = <eol> = = = <unk> = =\n",
            "Input | Output #2: 25 April 2013 , denoting shipments of 500 @,@ 000 copies . <eol> The song became One Direction 's fourth | single , and the first single in the United States\n",
            "Input | Output #3: , and Bruce R. ) one daughter ( Wendy J. <unk> ) and two grandchildren , died in <unk> , | <unk> , and the <unk> of the <unk> <unk> <unk>\n",
            "Input | Output #4: Warrior were examples of this type . Because their armor was so heavy , they could only carry a single | <unk> . <eol> = = = <unk> = = =\n",
            "Input | Output #5: the embassy at 1 : 49 and landed on Guam at 2 : 23 ; twenty minutes later , Ambassador | in the United States . <eol> = = = <unk>\n",
            "Input | Output #6: <unk> , $ 96 million USD ) . Damage was heaviest in South Korea , notably where it moved ashore | . The ship was completed in the United States in\n",
            "Input | Output #7: The <unk> were condemned as <unk> by <unk> , who saw the riots as hampering attempts to resolve the situation | . <eol> = = = <unk> = = = <eol>\n",
            "Input | Output #8: by a decision made by the War Office in mid @-@ 1941 , as it was considering the equipment to | be the first @-@ time field of the two .\n",
            "Input | Output #9: Division crossed the <unk> at a number of places and climbed the hills quietly toward the 9th Infantry river line | . <eol> = = = <unk> = = = <eol>\n",
            "Input | Output #10: = <eol> = = = French VIII . Corps ( Corps <unk> ) = = = <eol> On 6 November | , the route was completed in the United States in\n",
            "Input | Output #11: of the World from 9th Avenue \" . This is regarded as his most famous work . It is considered | to be a <unk> of the <unk> of the <unk>\n",
            "Input | Output #12: — <unk> @-@ 10 , <unk> @-@ 12 , <unk> @-@ 16 , <unk> @-@ 17 — were all converted | to the village . The ship was completed in the\n",
            "Input | Output #13: And now he has . \" <eol> = = Family = = <eol> <unk> lived 37 of his years in | the United States , and the first of the two\n",
            "Input | Output #14: Hell to which he has been condemned for <unk> . Eliot , in a letter to John <unk> dated 27 | December , was the first of the most important of\n",
            "Input | Output #15: Luoyang area , fulfilling his duties in domestic affairs . <eol> In the autumn of <unk> , he met Li | <unk> , who was appointed the \" <unk> \" and\n",
            "Input | Output #16: Power said they enjoyed Block Ball and its number of stages , but wondered how its eight <unk> of memory | was not to be the most popular . \" <eol>\n",
            "Input | Output #17: by Lloyd F. Lonergan . The cameraman was Jacques <unk> . <eol> = = Release and reception = = <eol> | = = = <unk> = = = <eol> The film\n",
            "Input | Output #18: alone , the Austrians lost more than half their reserve artillery park , 6 @,@ 000 ( out of 8 | ) and the <unk> <unk> ( <unk> ) . The\n",
            "Input | Output #19: while attacking a ship at <unk> in the Dutch East Indies ; the loss was compounded by the fact that | the war was not to be the <unk> of the\n",
            "Input | Output #20: first raised in 2007 by the member of parliament ( MP ) for <unk> . The gangsters may have run | to the <unk> of the <unk> . The <unk> is\n",
            "Input | Output #21: Species are also non @-@ spiny <unk> and includes both large trees with stout stems up to 30 metres ( | 0 @.@ 5 in ) . <eol> = = =\n",
            "Input | Output #22: \" : specific design issues with the building 's energy efficiency included the fact that the largest room in the | world is not a <unk> . \" <eol> = =\n",
            "Input | Output #23: were reported to support over 300 @,@ 000 households in the Brazilian state of <unk> in 2005 , and in | the early 19th century , the kakapo was also used\n",
            "Input | Output #24: port . <unk> in Vietnam also warned for the potential of heavy rainfall due to the dissipating Tropical Depression <unk> | . The ship was completed in the United States in\n",
            "Input | Output #25: T @-@ numbers in their tropical cyclone products . The following example is from discussion number 3 of Tropical Depression | <unk> of the <unk> <unk> <unk> ( <unk> ) ,\n",
            "Input | Output #26: South Australia hosted the three @-@ game semi @-@ final series against the New South Wales <unk> . Both teams | were also named the song 's first career in the\n",
            "Input | Output #27: Perth from contention and secured the last finals spot for the <unk> . <eol> = = = Statistical leaders = | = = <eol> = = = <unk> = = =\n",
            "Input | Output #28: deemed it an \" amazing pop song \" , lauding the group 's falsetto and its \" head @-@ <unk> | \" . <eol> = = = <unk> = = =\n",
            "Input | Output #29: , but began patrolling the English Channel after <unk> @-@ 6 pioneered a route past British <unk> nets and mines | in the village of <unk> . The ship was completed\n",
            "Input | Output #30: production executives to let him direct . He had already discussed the film with <unk> and Cohen , and felt | that the song was \" <unk> \" . <eol> =\n",
            "Input | Output #31: and Nick <unk> at Studio <unk> in Los Angeles , California , and was released on August 1 , 2006 | . <eol> = = = <unk> = = = <eol>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# see generated output\n",
        "print (trainer.generated[-1]) # get last generated output"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "neel_hw4p1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

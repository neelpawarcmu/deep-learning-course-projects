{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PcC2d2KAkog2"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neelpawarcmu/deep-learning-course-projects/blob/main/Utterance_to_Phoneme_Mapping_using_Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "**Task: Unaligned Phoneme Recognition / Utterance to Phoneme Mapping.**\n",
        "\n",
        "We are given unaligned labels, which means the correlation between the features and labels is not given explicitly and our model will have to figure this out by itself. The data has a list of phonemes for each utterance, but not which frames correspond to which phonemes. The main task is to predict the phonemes contained in utterances in the test set. We are not given aligned phonemes in the training data, and we are not producing alignment for the test data.\n",
        "\n",
        "\\\n",
        "To solve this problem we build a sequence to sequence SpeechNet model from scratch using methodology and architectural details of sequence to sequence modeling mentioned in [this research paper](https://arxiv.org/pdf/2105.03070.pdf). We use CTC decode to solve the unaligned feature and label problem.\n",
        "\n",
        "\n",
        "\\\n",
        "Additional details can be found [here](https://www.kaggle.com/competitions/11785-fall2021-hw3p2/data)"
      ],
      "metadata": {
        "id": "GMHbC3UTsADQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqxhejj_yxxo"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPhXLOOGy23O"
      },
      "source": [
        "## imports and configs \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Google Drive, Kaggle Preprocessing"
      ],
      "metadata": {
        "id": "SMWoBCwerzQE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e15Pr4YZyzx6",
        "outputId": "99d8ad8f-29e0-4ef9-9c8c-6fd3746438b7"
      },
      "source": [
        "# Google drive setup\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGghymIKyzkY"
      },
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# dont load dataset if already loaded\n",
        "def setupNeeded():\n",
        "  if os.path.exists('data'):\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n",
        "if setupNeeded():\n",
        "  print('\\n-----------Installing kaggle----------\\n')\n",
        "  # make kaggle dir and initialize the api token for downloading to the dir \n",
        "  # if this step gives a 401 error: go to kaggle and generate new api token from account settings\n",
        "  api_token = {\"username\":\"neelpawarcmu\",\"key\":\"6b3a4829599c17cc4fac80794b01329d\"} \n",
        "  !mkdir .kaggle\n",
        "  !mkdir ~/.kaggle\n",
        "  with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "  !cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "  !chmod 600 /root/.kaggle/kaggle.json\n",
        "  \n",
        "  # kaggle install\n",
        "  !pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "  !kaggle --version\n",
        "  \n",
        "  # download data from the kaggle competition\n",
        "  print('\\n-----------Downloading data----------\\n')\n",
        "  !kaggle competitions download -c 11785-fall2021-hw3p2 # <- change this to download from a different kaggle competition\n",
        "  !mkdir data\n",
        "  print('\\n-----------Unzipping data----------\\n')\n",
        "  !unzip -qo './11785-fall2021-hw3p2.zip' -d data \n",
        "  !ls data/\n",
        "\n",
        "  # clone ctcdecoder (pip install ctcdecode causes installation errors)\n",
        "  print('\\n-----------Downloading ctcdecode----------\\n')\n",
        "  !git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "  !pip install wget\n",
        "  %cd ctcdecode\n",
        "\n",
        "  !pip install .\n",
        "  %cd ..\n",
        "\n",
        "  # install Levenshtein\n",
        "  print('\\n-----------Downloading Levenshtein----------\\n')\n",
        "  !pip install python-Levenshtein\n",
        "  print('\\n-----------All prerequisites downloaded----------\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yqlnQRA0DZk"
      },
      "source": [
        "#### Libraries & Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAHPFFCc0C7b"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import Levenshtein\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pdb\n",
        "import gc\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# cudnn autotuner to speed up cnns\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvSpD3_q0UHH",
        "outputId": "bc58fc3c-b69a-4c29-96e0-7620979f10be"
      },
      "source": [
        "# Check if cuda is available and set device\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "NUM_WORKERS = 8 if cuda else 0\n",
        "\n",
        "print(\"Cuda = \", str(cuda), \" with num_workers = \", str(NUM_WORKERS),  \" system version = \", sys.version)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda =  True  with num_workers =  8  system version =  3.7.13 (default, Apr 24 2022, 01:04:09) \n",
            "[GCC 7.5.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YopAeO_XRhTQ"
      },
      "source": [
        "## Data loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKo7GEsBa4sR"
      },
      "source": [
        "### Load raw data from unzipped files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BDZrzzlF5sq"
      },
      "source": [
        "# load training and dev data\n",
        "train_data = np.load('data/HW3P2_Data/train.npy', allow_pickle=True)\n",
        "train_labels = np.load('data/HW3P2_Data/train_labels.npy', allow_pickle=True)\n",
        "\n",
        "dev_data = np.load('data/HW3P2_Data/dev.npy', allow_pickle=True)\n",
        "dev_labels = np.load('data/HW3P2_Data/dev_labels.npy', allow_pickle=True)\n",
        "\n",
        "# load test data\n",
        "test_data = np.load('data/HW3P2_Data/test.npy', allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rf89XpGrmYHq",
        "outputId": "b6dbc018-a3b8-40f4-f481-1e3c421dc6f8"
      },
      "source": [
        "print(f'Train data: {train_data.shape}')\n",
        "print(f'Train labels {train_labels.shape}')\n",
        "\n",
        "print(f'Dev data: {dev_data.shape}')\n",
        "print(f'Dev labels {dev_labels.shape}')\n",
        "\n",
        "print(f'Test data: {test_data.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data: (14542,)\n",
            "Train labels (14542,)\n",
            "Dev data: (2200,)\n",
            "Dev labels (2200,)\n",
            "Test data: (2561,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  print(train_data[i].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug-NppKEeeUe",
        "outputId": "a49b5cd8-5f5e-449a-a1f1-3374fdc15c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1504, 40)\n",
            "(1560, 40)\n",
            "(566, 40)\n",
            "(1453, 40)\n",
            "(795, 40)\n",
            "(488, 40)\n",
            "(1261, 40)\n",
            "(1544, 40)\n",
            "(1314, 40)\n",
            "(1479, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vki5P3jzT36M"
      },
      "source": [
        "### Create Toy Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu_C1bUWBSiK"
      },
      "source": [
        "train_data_toy = train_data[:1000]\n",
        "train_labels_toy = train_labels[:1000]\n",
        "\n",
        "dev_data_toy = dev_data[:200]\n",
        "dev_labels_toy = dev_labels[:200]\n",
        "\n",
        "test_data_toy = test_data[:200]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ssO-TJFBXhl",
        "outputId": "2b5d0395-d15e-4288-c015-f0125c7e8a3c"
      },
      "source": [
        "print(f'Train data toy: {train_data_toy.shape}')\n",
        "print(f'Train labels toy: {train_labels_toy.shape}')\n",
        "\n",
        "print(f'Dev data toy: {dev_data_toy.shape}')\n",
        "print(f'Dev data toy: {dev_data_toy.shape}')\n",
        "\n",
        "print(f'Test labels toy: {dev_labels_toy.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data toy: (1000,)\n",
            "Train labels toy: (1000,)\n",
            "Dev data toy: (200,)\n",
            "Dev data toy: (200,)\n",
            "Test labels toy: (200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3xxesV3BiUy"
      },
      "source": [
        "# save training and dev data\n",
        "# TODO: Edit path to where you want to save data\n",
        "# train_data = np.load('/content/gdrive/MyDrive/.../train_data_toy.npy', allow_pickle=True)\n",
        "# train_labels = np.load('/content/gdrive/MyDrive/.../train_labels_toy.npy', allow_pickle=True)\n",
        "\n",
        "# dev_data = np.load('/content/gdrive/MyDrive/.../dev_data_toy.npy', allow_pickle=True)\n",
        "# dev_labels = np.load('/content/gdrive/MyDrive/.../dev_labels_toy.npy', allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iLs7aLgbB8Q"
      },
      "source": [
        "### Create Custom Dataset Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-0Seqh4Cp9F"
      },
      "source": [
        "# Define dataset class\n",
        "class MyDataSet(Dataset):\n",
        "  '''\n",
        "  define train and validation dataset class for the torch dataloader to serve from\n",
        "  '''\n",
        "  # load the dataset\n",
        "  def __init__(self, x, y):\n",
        "    self.X = x\n",
        "    self.Y = y\n",
        "\n",
        "  # get number of items/rows in dataset\n",
        "  def __len__(self):\n",
        "    return len(self.Y)\n",
        "\n",
        "  # get row item at some index\n",
        "  def __getitem__(self, index):\n",
        "    x = torch.FloatTensor(self.X[index])\n",
        "    y = torch.LongTensor(self.Y[index])\n",
        "    return x, y\n",
        "\n",
        "  def collate_fn(batch):\n",
        "    batch_x = [x for x,y in batch]\n",
        "    batch_y = [y for x,y in batch]\n",
        "    lengths_x = torch.as_tensor([len(x) for x in batch_x])\n",
        "    lengths_y = torch.as_tensor([len(y) for y in batch_y])\n",
        "    padded_batch_x = pad_sequence(batch_x, batch_first=True)\n",
        "    padded_batch_y = pad_sequence(batch_y, batch_first=True)\n",
        "    return padded_batch_x, padded_batch_y, lengths_x, lengths_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYTDtHEGHsWZ"
      },
      "source": [
        "# Define dataset class\n",
        "class TestDataSet(Dataset):\n",
        "  '''\n",
        "  \n",
        "  '''\n",
        "  # load the dataset\n",
        "  # TODO: replace x and y with dataset path and load data from here -> more efficient\n",
        "  def __init__(self, x):\n",
        "    self.X = x\n",
        "\n",
        "  # get number of items/rows in dataset\n",
        "  def __len__(self):\n",
        "    return len(self.X) \n",
        "\n",
        "  # get row item at some index\n",
        "  def __getitem__(self, index):\n",
        "    x = torch.FloatTensor(self.X[index])\n",
        "    return x\n",
        "\n",
        "  def collate_fn(batch_x):\n",
        "    lengths_x = torch.as_tensor([len(x) for x in batch_x])\n",
        "    padded_batch_x = pad_sequence(batch_x, batch_first=True, padding_value=0)\n",
        "    return padded_batch_x, lengths_x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4ppavAxbGYv"
      },
      "source": [
        "## 2.3 Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_knptYW7RR73"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset_in_use = 'toy'\n",
        "# dataset_in_use = 'main'\n",
        "\n",
        "if dataset_in_use == 'toy':\n",
        "  train_data, train_labels, dev_data, dev_labels, test_data = train_data_toy, train_labels_toy, dev_data_toy, dev_labels_toy, test_data_toy\n",
        "\n",
        "\n",
        "# training data\n",
        "train = MyDataSet(train_data, train_labels)\n",
        "train_args = dict(shuffle = True, batch_size = BATCH_SIZE, num_workers=NUM_WORKERS, collate_fn=MyDataSet.collate_fn)\n",
        "train_loader = DataLoader(train, **train_args)\n",
        "\n",
        "# validation data\n",
        "dev = MyDataSet(dev_data, dev_labels)\n",
        "dev_args = dict(shuffle = False, batch_size = BATCH_SIZE, num_workers=NUM_WORKERS, collate_fn=MyDataSet.collate_fn) \n",
        "dev_loader = DataLoader(dev, **dev_args)\n",
        "\n",
        "# test data\n",
        "test = TestDataSet(test_data)\n",
        "test_args = dict(shuffle = False, batch_size = BATCH_SIZE, num_workers=NUM_WORKERS, collate_fn=TestDataSet.collate_fn)\n",
        "test_loader = DataLoader(test, **test_args)\n",
        "\n",
        "test_toy = TestDataSet(test_data_toy)\n",
        "test_loader_toy = DataLoader(test_toy, **test_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaJEBFMBBZ7D"
      },
      "source": [
        "# # save test data:\n",
        "# torch.save(test_loader_toy, \"/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/test_loader.pt\")\n",
        "# torch.save(test_toy, \"/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/test_dataset.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BqIbX5pOvJ0"
      },
      "source": [
        "# save data\n",
        "savedata = False\n",
        "if savedata:\n",
        "  torch.save(train, \"/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/train_dataset.pt\")\n",
        "  torch.save(train_labels, \"/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/train_labels_dataset.pt\")\n",
        "  torch.save(dev, \"/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/dev_dataset.pt\")\n",
        "  torch.save(dev_labels, \"/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/dev_labels_dataset.pt\")\n",
        "  torch.save(test, \"/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/test_dataset.pt\")\n",
        "\n",
        "saveloader = False\n",
        "if saveloader:\n",
        "  torch.save(train_loader, \"/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/train_loader.pt\")\n",
        "  torch.save(dev_loader, \"/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/dev_loader.pt\")\n",
        "  torch.save(test_loader, \"/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/test_loader.pt\")\n",
        "\n",
        "#load data\n",
        "loaddata = False\n",
        "if loaddata:\n",
        "  train_data = torch.load('/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/train_dataset.pt')\n",
        "  train_labels = torch.load(\"/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/train_labels_dataset.pt\")\n",
        "  dev_data = torch.load(\"/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/dev_dataset.pt\")\n",
        "  dev_labels = torch.load(\"/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/dev_labels_dataset.pt\")\n",
        "  test_data = torch.load(\"/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/test_dataset.pt\")\n",
        "\n",
        "\n",
        "loadloader = True\n",
        "if loadloader:\n",
        "  train_loader = torch.load('/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/train_loader.pt')\n",
        "  dev_loader = torch.load(\"/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/dev_loader.pt\")\n",
        "  test_loader = torch.load(\"/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/test_loader.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which data has been loaded \n",
        "\n",
        "print('------------------------------------')\n",
        "print('Active dataset:\\n')\n",
        "print(f'number of examples in train data: {len(train_data)}')\n",
        "print(f'number of examples in val data: {len(dev_data)}')\n",
        "print(f'number of examples in test data: {len(test_data)}')\n",
        "\n",
        "print('train_loader length (in batches)', len(train_loader))\n",
        "print('------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3_p4zVOc8yQ",
        "outputId": "85304ac2-db9e-49ef-8773-6a4ba1956689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------\n",
            "Active dataset:\n",
            "\n",
            "number of examples in train data: 1000\n",
            "number of examples in val data: 200\n",
            "number of examples in test data: 200\n",
            "train_loader length (in batches) 16\n",
            "------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQ1dOqiWUyyg",
        "outputId": "73b04ed8-ebcd-4800-8e64-2fa64789b80a"
      },
      "source": [
        "#test one batch of dataloader:\n",
        "trial_x = trial_y = None\n",
        "for batch, (x,y, len_x, len_y) in enumerate(train_loader): #batch, (x,y) in enumerate(train_loader):\n",
        "  break\n",
        "\n",
        "print('\\n------------------------------------\\n')\n",
        "print(f'batches in :\\ttrain_loader:{len(train_loader)},\\tdev_loader:{len(dev_loader)},\\ttest_loader:{len(test_loader)}\\n')\n",
        "print(f'batch {batch} of train_loader:\\n')\n",
        "print(f'x: {x.shape}')\n",
        "print(f'y: {y.shape}')\n",
        "print(f'len_x: {len(len_x)}')\n",
        "print(f'len_y: {len(len_y)}')\n",
        "print('\\n------------------------------------\\n')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------------------------\n",
            "\n",
            "batches in :\ttrain_loader:16,\tdev_loader:4,\ttest_loader:4\n",
            "\n",
            "batch 0 of train_loader:\n",
            "\n",
            "x: torch.Size([64, 1723, 40])\n",
            "y: torch.Size([64, 198])\n",
            "len_x: 64\n",
            "len_y: 64\n",
            "\n",
            "------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvPJa4EBkTEp",
        "outputId": "ec3fe593-2058-4c9c-c641-dc5b7e4d48b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-4.2428,  1.9185,  0.6261,  ...,  2.7999, -9.4368,  0.6930],\n",
              "        [ 1.7173,  1.2406, -0.6536,  ...,  1.9749, -1.1662,  0.1333],\n",
              "        [-1.9295, -0.1151,  0.3308,  ...,  2.6052,  0.3467,  0.6405],\n",
              "        ...,\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt_fIhfDbMMm"
      },
      "source": [
        "# 2 Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSSX7kU5XBy_"
      },
      "source": [
        "## 2.1 Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aby-1ILgpRMD"
      },
      "source": [
        "# create building blocks\n",
        "\n",
        "\n",
        "# Convolutional - Batchnorm - ReLU block\n",
        "class CBR(nn.Sequential):\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super().__init__(\n",
        "        nn.Conv1d(input_size, output_size, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm1d(output_size),\n",
        "        nn.ReLU(output_size)\n",
        "    )\n",
        "\n",
        "\n",
        "# Convolutional - Batchnorm - ELU block\n",
        "class CBE(nn.Sequential):\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super().__init__(\n",
        "        nn.Conv1d(input_size, output_size, kernel_size=3, padding=1, bias=False),\n",
        "        nn.BatchNorm1d(output_size),\n",
        "        nn.ELU()\n",
        "    )\n",
        "\n",
        "\n",
        "# Convolutional Block (design 1) by stacking CBR blocks\n",
        "class ConvBlock(nn.Sequential):\n",
        "  def __init__(self, input_size, narrow, wide):\n",
        "    super().__init__(\n",
        "        CBR(input_size, narrow),\n",
        "        CBR(narrow, narrow),\n",
        "        CBR(narrow, wide),\n",
        "        CBR(wide, wide),\n",
        "        CBR(wide, wide)\n",
        "    )\n",
        "\n",
        "\n",
        "# Convolutional Block (design 2) by stacking CBR blocks\n",
        "class ConvBlockWide(nn.Sequential):\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super().__init__(\n",
        "        CBE(input_size, output_size)\n",
        "    )  \n",
        "\n",
        "\n",
        "# LSTM Block\n",
        "class LSTMModel(nn.Sequential):\n",
        "  def __init__(self, embedding_size, lstm_output_size, num_lstm_layers, bi, dropout):\n",
        "    super().__init__(\n",
        "        nn.LSTM(embedding_size, lstm_output_size, num_lstm_layers, bidirectional=bi, batch_first=True, dropout=dropout)\n",
        "    )\n",
        "\n",
        "\n",
        "# Final Linear MLP Block\n",
        "class LinearBlock(nn.Sequential):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super().__init__(\n",
        "        nn.Linear(input_size, hidden_size),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(hidden_size, output_size)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG3wNcq1pJWU"
      },
      "source": [
        "## My Seq2Seq model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIfhWfNjqiTz"
      },
      "source": [
        "# Wide model\n",
        "class WideSeq2SeqModel(nn.Module):\n",
        "  def __init__(self, input_size, conv_output, lstm_units, lstm_layers, bi, linear_hidden, output_size, dropout=0.2):\n",
        "    lstm_directions = 2 if bi else 1\n",
        "    linear_input = lstm_units * lstm_directions\n",
        "    \n",
        "    super().__init__()\n",
        "    self.conv = ConvBlockWide(input_size, conv_output)\n",
        "    self.lstm = LSTMModel(conv_output, lstm_units, num_lstm_layers=lstm_layers, bi=bi, dropout=dropout)\n",
        "    self.linear = LinearBlock(linear_input, linear_hidden, output_size)\n",
        "\n",
        "  '''\n",
        "  SHAPE CALCULATIONS FOR SHAPE MATCHING IN WORKFLOW\n",
        "    if dataloader has batchfirst true: (batches, length, channels)\n",
        "    input and output of cnn: from conv1d docs (batches, channels, length) -> permute(0,2,1)\n",
        "    input of pack padded: from pack_padded_sequence docs (length, batches, channels) -> permute(2,0,1)\n",
        "    output of pack padded w/o batchfirst: from \" docs (length, batches, channels)\n",
        "    input of lstm: from nn.lstm docs (batches, length, channels) -> permute(1,0,2)\n",
        "    input of pad packed w batch first: from pad_packed docs (batches, length, channels) \n",
        "    output of pad packed w batch first: \" (batches, length, channels)\n",
        "    linear: (couldnt find details in docs): same,  (batches, length, channels) \n",
        "    ctcloss input ie model final output:  (length, batches, channels) -> permute(1,0,2)\n",
        "  '''\n",
        "\n",
        "  def forward(self, x, x_lengths): # x dim (batches, Seq_len, Channels=40)\n",
        "    x_cnn_input = x.permute(0, 2, 1) # (batches, in_channels, length)\n",
        "    x_post_cnn = self.conv(x_cnn_input) # (batches, out_channels, length_out)\n",
        "    x_rnn_in = x_post_cnn.permute(2, 0, 1) # (length_out, batches, channels_out)\n",
        "    x_packed = pack_padded_sequence(x_rnn_in, x_lengths, enforce_sorted=False)\n",
        "    out_packed, hidden = self.lstm(x_packed)\n",
        "    out, out_lens = pad_packed_sequence(out_packed, batch_first=True) # (batches, length_out, out_channels)\n",
        "    # Log softmax after output layer is required since nn.CTCLoss expect log prob\n",
        "    out_prob = self.linear(out).log_softmax(2) # (batches, length, out_channels)\n",
        "    # Permute to fit for input format of CTCLoss\n",
        "    out_prob = out_prob.permute(1, 0, 2) #torch.transpose(out_prob, 0, 1) # (lengths, batches, out_channels)\n",
        "    return out_prob, x_lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RSFISlRphHU"
      },
      "source": [
        "# Narrow model\n",
        "class Seq2SeqModel(nn.Module):\n",
        "  def __init__(self, input_size, conv_hiddens, lstm_units, lstm_layers, bi, linear_hidden, output_size, dropout=0.2):\n",
        "    directions = 2 if bi else 1\n",
        "    linear_input = lstm_units * directions\n",
        "    super().__init__()\n",
        "    self.conv = ConvBlock(40, conv_hiddens[0], conv_hiddens[1])\n",
        "    self.lstm = LSTMModel(conv_hiddens[1], lstm_units, num_lstm_layers=lstm_layers, bi=bi, dropout=dropout)    \n",
        "    self.linear = LinearBlock(linear_input, linear_hidden, output_size)\n",
        "\n",
        "  def forward(self, x, xLens): # x dim (B, T_in, C_in=40)\n",
        "    x_cnn_input = x.permute(0, 2, 1) # (B, C_in, T_in)\n",
        "    x_post_cnn = self.conv(x_cnn_input) # (B, C_out, T_out)\n",
        "    x_rnn_in = x_post_cnn.permute(2, 0, 1) # (T, B, C_out)\n",
        "    x_packed = pack_padded_sequence(x_rnn_in, xLens, enforce_sorted=False)\n",
        "    out_packed, hidden = self.lstm(x_packed)\n",
        "    out, out_lens = pad_packed_sequence(out_packed, batch_first=True) # (B, T, C)\n",
        "    \n",
        "    # Log softmax after output layer is required since nn.CTCLoss expect log prob\n",
        "    out_prob = self.linear(out).log_softmax(2) # (B, T, Classes=47)\n",
        "    \n",
        "    # Permute to fit for input format of CTCLoss\n",
        "    out_prob = out_prob.permute(1, 0, 2) #torch.transpose(out_prob, 0, 1) # (T, B, C)\n",
        "    \n",
        "    # calculate new xLens\n",
        "    return out_prob, xLens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_2OrOTObTEy"
      },
      "source": [
        "## 2.2 Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4q8xJFgnZT1v",
        "outputId": "0f11640b-2a41-4bb1-aea4-85fd4427aa35"
      },
      "source": [
        "# create model\n",
        "input_size = 40\n",
        "conv_hiddens = [128, 256]\n",
        "lstm_units = 512\n",
        "lstm_layers = 4\n",
        "bidirectional = True\n",
        "linear_hidden = 512\n",
        "output_size = 42\n",
        "\n",
        "\n",
        "\n",
        "wide_model = WideSeq2SeqModel(input_size, 1024, lstm_units,  lstm_layers, bidirectional, linear_hidden, output_size, dropout=0.5)\n",
        "narrow_model = Seq2SeqModel(input_size, conv_hiddens, lstm_units,  lstm_layers, bidirectional, linear_hidden, output_size, dropout=0.2) \n",
        "\n",
        "# SELECT MODEL\n",
        "model = wide_model\n",
        "model_path = '/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_models/test_best_model.pt'\n",
        "##########\n",
        "\n",
        "try:\n",
        "  model.load_state_dict(torch.load(model_path))\n",
        "  print('√√√√√√√√√√√√√√ model keys matched √√√√√√√√√√√√√√\\n\\n')\n",
        "except:\n",
        "  print('XXXXXXXXXXXXX didnt load any model XXXXXXXXXXXXX\\n\\n')\n",
        "model = model.to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "√√√√√√√√√√√√√√ model keys matched √√√√√√√√√√√√√√\n",
            "\n",
            "\n",
            "WideSeq2SeqModel(\n",
            "  (conv): ConvBlockWide(\n",
            "    (0): CBE(\n",
            "      (0): Conv1d(40, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
            "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ELU(alpha=1.0)\n",
            "    )\n",
            "  )\n",
            "  (lstm): LSTMModel(\n",
            "    (0): LSTM(1024, 512, num_layers=4, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  )\n",
            "  (linear): LinearBlock(\n",
            "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=512, out_features=42, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1hBJnqPjxyu"
      },
      "source": [
        "# 4 Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azt5PaudbbmK"
      },
      "source": [
        "## 4.0 Set Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6cbZIpDBWFa"
      },
      "source": [
        "# Hyperparams\n",
        "LEARNING_RATE = 5e-4\n",
        "WEIGHT_DECAY = 5e-5\n",
        "\n",
        "\n",
        "criterion = nn.CTCLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.8, patience=2, verbose=True)\n",
        "\n",
        "# scheduler alternative (Adam gives decnt results in most cases)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWLIH1eNbjtg"
      },
      "source": [
        "## 4.1 Train Epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtUS0ck-aC5-"
      },
      "source": [
        "# Train the model\n",
        "def train_epoch(model, train_loader, criterion, optimizer):\n",
        "  # put model in train mode\n",
        "  model.train()\n",
        "\n",
        "  # initialize loss and timer for one epoch\n",
        "  avg_loss = 0.0\n",
        "  start = time.time()\n",
        "\n",
        "  # initialize timer to time every batch (used to tune speeds before final deployment)\n",
        "  start_time = time.time()\n",
        "  for batch, (x,y,len_x,len_y) in enumerate(train_loader):\n",
        "    # reset gradients to zero, required before starting backpropragation\n",
        "    optimizer.zero_grad()\n",
        "    # move all tensors to one device\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    # generate model output\n",
        "    output, len_output = model(x, len_x) # two outputs due to LSTM \n",
        "    output.to(device)\n",
        "    # backprop\n",
        "    loss = criterion(output, y, len_x, len_y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # annotate time and loss\n",
        "    if batch and batch % 10 == 0:\n",
        "        end_time = time.time()\n",
        "        print(f\"batch: {batch} \\t lr : {optimizer.param_groups[0]['lr']} \\t training loss : {loss} \\t time taken : {(end_time-start_time)*10//1/10} sec\")\n",
        "        start_time = end_time\n",
        "    # clear memory\n",
        "    torch.cuda.empty_cache()\n",
        "    del x\n",
        "    del y\n",
        "    del len_x\n",
        "    del len_y\n",
        "    del output\n",
        "    del len_output\n",
        "\n",
        "  return loss\n",
        "\n",
        "  # timing and loss for entire epoch\n",
        "  end = time.time()\n",
        "  avg_loss /= len(train_loader) # average batch loss\n",
        "\n",
        "  print(f'Training loss: {avg_loss} Time: {end - start}')\n",
        "  return avg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idmvOyiEq63H"
      },
      "source": [
        "## 4.2 CTC Decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrIW7OENr7fg",
        "outputId": "f3d06e9e-6eba-4207-dbd1-18a4f3580157"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"data/HW3P2_Data\")\n",
        "sys.path.append('/content/gdrive/MyDrive/IDL-Kaggle/hw3')\n",
        "\n",
        "from phoneme_list import PHONEME_MAP, PHONEME_LIST\n",
        "print(PHONEME_LIST)\n",
        "print(PHONEME_MAP)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' ', 'SIL', 'SPN', 'AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH', 'EH', 'ER', 'EY', 'F', 'G', 'H', 'IH', 'IY', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW', 'OY', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH', 'UW', 'V', 'W', 'Y', 'Z', 'ZH']\n",
            "[' ', '.', '!', 'a', 'A', 'h', 'o', 'w', 'y', 'b', 'c', 'd', 'D', 'e', 'r', 'E', 'f', 'g', 'H', 'i', 'I', 'j', 'k', 'l', 'm', 'n', 'N', 'O', 'Y', 'p', 'R', 's', 'S', 't', 'T', 'u', 'U', 'v', 'W', '?', 'z', 'Z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mby2lwGsX-a"
      },
      "source": [
        "from ctcdecode import CTCBeamDecoder\n",
        "\n",
        "\n",
        "class Decoder(CTCBeamDecoder):\n",
        "  def __init__(self, beam_width=100):\n",
        "    self.decoder = CTCBeamDecoder(labels=PHONEME_MAP, beam_width=beam_width, log_probs_input=True)\n",
        "\n",
        "  def decode(self, output, len_x): \n",
        "    # print('output unprocessed', output.shape)\n",
        "    output = torch.transpose(output, 0, 1)\n",
        "    beam_results, beam_scores, timesteps, len_out = self.decoder.decode(output, len_x)\n",
        "    # print('beam_results', beam_results.shape)\n",
        "\n",
        "    #TCLO\n",
        "    decodedListShort = []\n",
        "    for b in range(beam_results.size(0)):\n",
        "        currDecode = \"\"\n",
        "        if len_out[b][0] != 0:\n",
        "            currDecodeShort = \"\".join([PHONEME_MAP[i] for i in beam_results[b, 0, :len_out[b][0]]])\n",
        "        decodedListShort.append(currDecodeShort)\n",
        "    return decodedListShort\n",
        "    \n",
        "    def get_edit_dist(self, output, output_lens, target, target_lens):\n",
        "      output, target = output.cpu(), target.cpu()\n",
        "      phonome_preds = self.decoder.decode(output, output_lens)\n",
        "      phonomes = self.decoder.convert_to_strings(target, target_lens)\n",
        "      edit_dist = np.sum([self.decoder.Lev_dist(phonome_pred, phonome) for (phonome_pred, phonome) in zip(phonome_preds, phonomes)])\n",
        "      return edit_dist\n",
        "  \n",
        "# Initialize decoder here\n",
        "decoder = Decoder()\n",
        "# In CTCBeamDecoder beam_width=1 (greedy search); beam_width>1 (beam search)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arxNVRFvj0ro"
      },
      "source": [
        "## 4.3 Validate Epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOQMIz_pQRat"
      },
      "source": [
        "def idx_to_phoneme(target):\n",
        "    return \"\".join([PHONEME_MAP[x] for x in target])\n",
        "\n",
        "def calculateLevScore(w1, w2):\n",
        "    return Levenshtein.distance(w1.replace(\" \", \"\"), w2.replace(\" \", \"\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SPnoWewQp1i"
      },
      "source": [
        "def validate_model(model, data_loader, epoch, decode=False):\n",
        "    with torch.no_grad(): # no grad reqd for backprop\n",
        "        # put model in eval mode\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        running_charErr = 0.0\n",
        "        totalSampleCnt = 0\n",
        "        print('validating', end=\"\")\n",
        "        for batch_idx, (data, target, dataLens, targetLens) in enumerate(data_loader):\n",
        "            print(\" –\", end=\"\")\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output, dataLens_new = model(data, dataLens)\n",
        "            loss = criterion(output,\n",
        "                             target,\n",
        "                             dataLens_new,\n",
        "                             targetLens)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            totalSampleCnt += len(data)\n",
        "            if decode:\n",
        "                decodedStringsShort = decoder.decode(output, dataLens)\n",
        "                targetStrings = [idx_to_phoneme(i) for i in target]\n",
        "                # print(decodedStringsShort)\n",
        "                # print(targetStrings)\n",
        "                \n",
        "                for i in range(len(targetStrings)):\n",
        "                    currCharErr = calculateLevScore(decodedStringsShort[i], targetStrings[i])\n",
        "                    running_charErr += currCharErr\n",
        "            \n",
        "            \n",
        "            torch.cuda.empty_cache()\n",
        "            del data\n",
        "            del target\n",
        "            del dataLens\n",
        "            del targetLens\n",
        "\n",
        "        loss_per_sample = running_loss / len(dev_data)\n",
        "        dist_per_sample = running_charErr / len(dev_data)\n",
        "        return loss_per_sample, dist_per_sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8vCpkiblZbY"
      },
      "source": [
        "## 4.4 Run Epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgoDMW3ISb6m"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjnPWR_MlDNZ",
        "outputId": "fe17f1f7-d48c-42fe-c581-69ec6d66cad9"
      },
      "source": [
        "# Define training settings\n",
        "epochs = 100\n",
        "epoch_array = np.arange(epochs)\n",
        "train_loss_array = np.zeros(epochs)\n",
        "val_loss_array = np.zeros(epochs)\n",
        "\n",
        "best_loss = float('inf')\n",
        "print('Start...')\n",
        "for epoch in range(epochs):\n",
        "  start_time = time.time()\n",
        "  print('Epoch: ', epoch+1)\n",
        "\n",
        "  training_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
        "  # val_loss, predictions, distance, running_dist = validate_model(model, dev_loader, epoch)\n",
        "  val_loss, running_dist = validate_model(model, dev_loader, epoch, decode=True)\n",
        "  \n",
        "  print(f'\\nEpoch {epoch+1} \\tval_loss {val_loss} \\tdistance {running_dist}')\n",
        "\n",
        "  # save the best model\n",
        "  if val_loss < best_loss:\n",
        "    print('Best loss: {}, epoch: {}'.format(val_loss, epoch + 1))\n",
        "    print('training loss: {}, epoch: {}'.format(val_loss, epoch + 1))\n",
        "    # update and save\n",
        "    model_path = '/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_models'\n",
        "    torch.save(model.state_dict(), os.path.join(model_path, 'test_best_model.pt'))\n",
        "    best_loss = val_loss\n",
        "\n",
        "  train_loss_array[epoch] = training_loss\n",
        "  val_loss_array[epoch] = val_loss\n",
        "\n",
        "  # step scheduler based on val loss\n",
        "  scheduler.step(val_loss)\n",
        "  print(f\"Epoch completed: {(time.time() - start_time)//6/10} min\")\n",
        "  print('LR = ', optimizer.param_groups[0]['lr'])\n",
        "  \n",
        "  print('='*40)\n",
        "print('Done...')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start...\n",
            "Epoch:  1\n",
            "batch: 10 \t lr : 0.0005 \t training loss : 4.948569297790527 \t time taken : 48.1 sec\n",
            "batch: 20 \t lr : 0.0005 \t training loss : 3.776538372039795 \t time taken : 40.8 sec\n",
            "batch: 30 \t lr : 0.0005 \t training loss : 3.476513385772705 \t time taken : 43.0 sec\n",
            "batch: 40 \t lr : 0.0005 \t training loss : 3.379917860031128 \t time taken : 39.7 sec\n",
            "batch: 50 \t lr : 0.0005 \t training loss : 3.3863964080810547 \t time taken : 42.6 sec\n",
            "batch: 60 \t lr : 0.0005 \t training loss : 3.344802141189575 \t time taken : 41.1 sec\n",
            "batch: 70 \t lr : 0.0005 \t training loss : 3.311263084411621 \t time taken : 42.9 sec\n",
            "batch: 80 \t lr : 0.0005 \t training loss : 3.2967472076416016 \t time taken : 38.5 sec\n",
            "batch: 90 \t lr : 0.0005 \t training loss : 3.2500076293945312 \t time taken : 39.3 sec\n",
            "batch: 100 \t lr : 0.0005 \t training loss : 3.2239456176757812 \t time taken : 41.6 sec\n",
            "batch: 110 \t lr : 0.0005 \t training loss : 3.218719959259033 \t time taken : 40.7 sec\n",
            "batch: 120 \t lr : 0.0005 \t training loss : 3.1949381828308105 \t time taken : 42.2 sec\n",
            "batch: 130 \t lr : 0.0005 \t training loss : 3.0539779663085938 \t time taken : 41.4 sec\n",
            "batch: 140 \t lr : 0.0005 \t training loss : 2.8415467739105225 \t time taken : 38.9 sec\n",
            "batch: 150 \t lr : 0.0005 \t training loss : 2.6469523906707764 \t time taken : 40.1 sec\n",
            "batch: 160 \t lr : 0.0005 \t training loss : 2.5818228721618652 \t time taken : 42.6 sec\n",
            "batch: 170 \t lr : 0.0005 \t training loss : 2.2391715049743652 \t time taken : 41.0 sec\n",
            "batch: 180 \t lr : 0.0005 \t training loss : 1.9310026168823242 \t time taken : 41.4 sec\n",
            "batch: 190 \t lr : 0.0005 \t training loss : 1.8478399515151978 \t time taken : 41.7 sec\n",
            "batch: 200 \t lr : 0.0005 \t training loss : 1.9612406492233276 \t time taken : 39.4 sec\n",
            "batch: 210 \t lr : 0.0005 \t training loss : 1.6441538333892822 \t time taken : 40.5 sec\n",
            "batch: 220 \t lr : 0.0005 \t training loss : 1.6975958347320557 \t time taken : 40.4 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 1 \tval_loss 0.023754887960173866 \tdistance 49.974545454545456\n",
            "Best loss: 0.023754887960173866, epoch: 1\n",
            "training loss: 0.023754887960173866, epoch: 1\n",
            "Epoch completed: 41.6 min\n",
            "LR =  0.0005\n",
            "========================================\n",
            "Epoch:  2\n",
            "batch: 10 \t lr : 0.0005 \t training loss : 1.4607256650924683 \t time taken : 48.6 sec\n",
            "batch: 20 \t lr : 0.0005 \t training loss : 1.297041654586792 \t time taken : 40.6 sec\n",
            "batch: 30 \t lr : 0.0005 \t training loss : 1.2121472358703613 \t time taken : 41.7 sec\n",
            "batch: 40 \t lr : 0.0005 \t training loss : 1.237064003944397 \t time taken : 41.8 sec\n",
            "batch: 50 \t lr : 0.0005 \t training loss : 0.8572256565093994 \t time taken : 41.0 sec\n",
            "batch: 60 \t lr : 0.0005 \t training loss : 0.8866207003593445 \t time taken : 43.3 sec\n",
            "batch: 70 \t lr : 0.0005 \t training loss : 0.8435519933700562 \t time taken : 38.9 sec\n",
            "batch: 80 \t lr : 0.0005 \t training loss : 0.7837250828742981 \t time taken : 40.9 sec\n",
            "batch: 90 \t lr : 0.0005 \t training loss : 0.7251101732254028 \t time taken : 40.1 sec\n",
            "batch: 100 \t lr : 0.0005 \t training loss : 0.7383093237876892 \t time taken : 38.8 sec\n",
            "batch: 110 \t lr : 0.0005 \t training loss : 0.723272979259491 \t time taken : 40.8 sec\n",
            "batch: 120 \t lr : 0.0005 \t training loss : 0.6807838082313538 \t time taken : 42.0 sec\n",
            "batch: 130 \t lr : 0.0005 \t training loss : 0.670174777507782 \t time taken : 39.3 sec\n",
            "batch: 140 \t lr : 0.0005 \t training loss : 0.5963640213012695 \t time taken : 39.3 sec\n",
            "batch: 150 \t lr : 0.0005 \t training loss : 0.6177397966384888 \t time taken : 40.4 sec\n",
            "batch: 160 \t lr : 0.0005 \t training loss : 0.6013442277908325 \t time taken : 44.1 sec\n",
            "batch: 170 \t lr : 0.0005 \t training loss : 0.6265308260917664 \t time taken : 40.8 sec\n",
            "batch: 180 \t lr : 0.0005 \t training loss : 0.5588443279266357 \t time taken : 39.1 sec\n",
            "batch: 190 \t lr : 0.0005 \t training loss : 0.5214707851409912 \t time taken : 39.9 sec\n",
            "batch: 200 \t lr : 0.0005 \t training loss : 0.551007091999054 \t time taken : 40.8 sec\n",
            "batch: 210 \t lr : 0.0005 \t training loss : 0.564744234085083 \t time taken : 39.2 sec\n",
            "batch: 220 \t lr : 0.0005 \t training loss : 0.514808177947998 \t time taken : 39.1 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 2 \tval_loss 0.008488754738460887 \tdistance 17.318636363636365\n",
            "Best loss: 0.008488754738460887, epoch: 2\n",
            "training loss: 0.008488754738460887, epoch: 2\n",
            "Epoch completed: 42.0 min\n",
            "LR =  0.0005\n",
            "========================================\n",
            "Epoch:  3\n",
            "batch: 10 \t lr : 0.0005 \t training loss : 0.4813179075717926 \t time taken : 46.6 sec\n",
            "batch: 20 \t lr : 0.0005 \t training loss : 0.5273573398590088 \t time taken : 40.1 sec\n",
            "batch: 30 \t lr : 0.0005 \t training loss : 0.43323132395744324 \t time taken : 39.7 sec\n",
            "batch: 40 \t lr : 0.0005 \t training loss : 0.43777745962142944 \t time taken : 40.4 sec\n",
            "batch: 50 \t lr : 0.0005 \t training loss : 0.42150503396987915 \t time taken : 42.2 sec\n",
            "batch: 60 \t lr : 0.0005 \t training loss : 0.34751367568969727 \t time taken : 39.1 sec\n",
            "batch: 70 \t lr : 0.0005 \t training loss : 0.3853152096271515 \t time taken : 39.2 sec\n",
            "batch: 80 \t lr : 0.0005 \t training loss : 0.38840505480766296 \t time taken : 40.1 sec\n",
            "batch: 90 \t lr : 0.0005 \t training loss : 0.37548092007637024 \t time taken : 39.8 sec\n",
            "batch: 100 \t lr : 0.0005 \t training loss : 0.37360864877700806 \t time taken : 38.6 sec\n",
            "batch: 110 \t lr : 0.0005 \t training loss : 0.3720068037509918 \t time taken : 41.5 sec\n",
            "batch: 120 \t lr : 0.0005 \t training loss : 0.43300846219062805 \t time taken : 40.0 sec\n",
            "batch: 130 \t lr : 0.0005 \t training loss : 0.3617996275424957 \t time taken : 40.6 sec\n",
            "batch: 140 \t lr : 0.0005 \t training loss : 0.510658323764801 \t time taken : 40.0 sec\n",
            "batch: 150 \t lr : 0.0005 \t training loss : 0.4387184679508209 \t time taken : 39.9 sec\n",
            "batch: 160 \t lr : 0.0005 \t training loss : 0.37698936462402344 \t time taken : 41.6 sec\n",
            "batch: 170 \t lr : 0.0005 \t training loss : 0.382712185382843 \t time taken : 39.5 sec\n",
            "batch: 180 \t lr : 0.0005 \t training loss : 0.35430920124053955 \t time taken : 43.0 sec\n",
            "batch: 190 \t lr : 0.0005 \t training loss : 0.3885546624660492 \t time taken : 39.6 sec\n",
            "batch: 200 \t lr : 0.0005 \t training loss : 0.42933472990989685 \t time taken : 42.5 sec\n",
            "batch: 210 \t lr : 0.0005 \t training loss : 0.36737966537475586 \t time taken : 41.9 sec\n",
            "batch: 220 \t lr : 0.0005 \t training loss : 0.3251761198043823 \t time taken : 41.4 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 3 \tval_loss 0.007038524868813428 \tdistance 14.181363636363637\n",
            "Best loss: 0.007038524868813428, epoch: 3\n",
            "training loss: 0.007038524868813428, epoch: 3\n",
            "Epoch completed: 42.4 min\n",
            "LR =  0.0005\n",
            "========================================\n",
            "Epoch:  4\n",
            "batch: 10 \t lr : 0.0005 \t training loss : 0.34501752257347107 \t time taken : 44.0 sec\n",
            "batch: 20 \t lr : 0.0005 \t training loss : 0.3254377245903015 \t time taken : 40.5 sec\n",
            "batch: 30 \t lr : 0.0005 \t training loss : 0.33253180980682373 \t time taken : 41.1 sec\n",
            "batch: 40 \t lr : 0.0005 \t training loss : 0.31362029910087585 \t time taken : 43.5 sec\n",
            "batch: 50 \t lr : 0.0005 \t training loss : 0.2977423071861267 \t time taken : 40.8 sec\n",
            "batch: 60 \t lr : 0.0005 \t training loss : 0.2934637665748596 \t time taken : 42.1 sec\n",
            "batch: 70 \t lr : 0.0005 \t training loss : 0.38776856660842896 \t time taken : 39.0 sec\n",
            "batch: 80 \t lr : 0.0005 \t training loss : 0.3936048150062561 \t time taken : 40.6 sec\n",
            "batch: 90 \t lr : 0.0005 \t training loss : 0.4200192093849182 \t time taken : 40.8 sec\n",
            "batch: 100 \t lr : 0.0005 \t training loss : 0.33701372146606445 \t time taken : 39.7 sec\n",
            "batch: 110 \t lr : 0.0005 \t training loss : 0.3982524871826172 \t time taken : 40.6 sec\n",
            "batch: 120 \t lr : 0.0005 \t training loss : 0.3631550073623657 \t time taken : 40.6 sec\n",
            "batch: 130 \t lr : 0.0005 \t training loss : 0.35315656661987305 \t time taken : 42.0 sec\n",
            "batch: 140 \t lr : 0.0005 \t training loss : 0.35136932134628296 \t time taken : 39.8 sec\n",
            "batch: 150 \t lr : 0.0005 \t training loss : 0.3287862539291382 \t time taken : 43.0 sec\n",
            "batch: 160 \t lr : 0.0005 \t training loss : 0.2912074029445648 \t time taken : 43.2 sec\n",
            "batch: 170 \t lr : 0.0005 \t training loss : 0.32060348987579346 \t time taken : 41.0 sec\n",
            "batch: 180 \t lr : 0.0005 \t training loss : 0.32514846324920654 \t time taken : 41.4 sec\n",
            "batch: 190 \t lr : 0.0005 \t training loss : 0.2975345849990845 \t time taken : 41.3 sec\n",
            "batch: 200 \t lr : 0.0005 \t training loss : 0.28532999753952026 \t time taken : 40.9 sec\n",
            "batch: 210 \t lr : 0.0005 \t training loss : 0.3226531744003296 \t time taken : 43.9 sec\n",
            "batch: 220 \t lr : 0.0005 \t training loss : 0.3004460632801056 \t time taken : 40.9 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 4 \tval_loss 0.006708392311226238 \tdistance 13.163181818181819\n",
            "Best loss: 0.006708392311226238, epoch: 4\n",
            "training loss: 0.006708392311226238, epoch: 4\n",
            "Epoch completed: 42.6 min\n",
            "LR =  0.0005\n",
            "========================================\n",
            "Epoch:  5\n",
            "batch: 10 \t lr : 0.0005 \t training loss : 0.28495389223098755 \t time taken : 46.2 sec\n",
            "batch: 20 \t lr : 0.0005 \t training loss : 0.27046141028404236 \t time taken : 41.3 sec\n",
            "batch: 30 \t lr : 0.0005 \t training loss : 0.27226322889328003 \t time taken : 39.9 sec\n",
            "batch: 40 \t lr : 0.0005 \t training loss : 0.30670085549354553 \t time taken : 38.5 sec\n",
            "batch: 50 \t lr : 0.0005 \t training loss : 0.2597169876098633 \t time taken : 44.2 sec\n",
            "batch: 60 \t lr : 0.0005 \t training loss : 0.2645958662033081 \t time taken : 40.7 sec\n",
            "batch: 70 \t lr : 0.0005 \t training loss : 0.2670419216156006 \t time taken : 39.5 sec\n",
            "batch: 80 \t lr : 0.0005 \t training loss : 0.27563515305519104 \t time taken : 42.0 sec\n",
            "batch: 90 \t lr : 0.0005 \t training loss : 0.3015021085739136 \t time taken : 43.8 sec\n",
            "batch: 100 \t lr : 0.0005 \t training loss : 0.29369795322418213 \t time taken : 39.6 sec\n",
            "batch: 110 \t lr : 0.0005 \t training loss : 0.2559257745742798 \t time taken : 39.4 sec\n",
            "batch: 120 \t lr : 0.0005 \t training loss : 0.3538955748081207 \t time taken : 40.8 sec\n",
            "batch: 130 \t lr : 0.0005 \t training loss : 0.2803782820701599 \t time taken : 41.4 sec\n",
            "batch: 140 \t lr : 0.0005 \t training loss : 0.30114662647247314 \t time taken : 41.4 sec\n",
            "batch: 150 \t lr : 0.0005 \t training loss : 0.28007346391677856 \t time taken : 41.2 sec\n",
            "batch: 160 \t lr : 0.0005 \t training loss : 0.28740033507347107 \t time taken : 40.0 sec\n",
            "batch: 170 \t lr : 0.0005 \t training loss : 0.3126085698604584 \t time taken : 38.8 sec\n",
            "batch: 180 \t lr : 0.0005 \t training loss : 0.27312207221984863 \t time taken : 41.5 sec\n",
            "batch: 190 \t lr : 0.0005 \t training loss : 0.2946273982524872 \t time taken : 39.9 sec\n",
            "batch: 200 \t lr : 0.0005 \t training loss : 0.2654151916503906 \t time taken : 41.9 sec\n",
            "batch: 210 \t lr : 0.0005 \t training loss : 0.3463233709335327 \t time taken : 41.2 sec\n",
            "batch: 220 \t lr : 0.0005 \t training loss : 0.31953859329223633 \t time taken : 42.5 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 5 \tval_loss 0.006457694267684763 \tdistance 12.626363636363637\n",
            "Best loss: 0.006457694267684763, epoch: 5\n",
            "training loss: 0.006457694267684763, epoch: 5\n",
            "Epoch completed: 42.8 min\n",
            "LR =  0.0005\n",
            "========================================\n",
            "Epoch:  6\n",
            "batch: 10 \t lr : 0.0005 \t training loss : 0.25724732875823975 \t time taken : 49.3 sec\n",
            "batch: 20 \t lr : 0.0005 \t training loss : 0.24470195174217224 \t time taken : 40.1 sec\n",
            "batch: 30 \t lr : 0.0005 \t training loss : 0.33417820930480957 \t time taken : 43.6 sec\n",
            "batch: 40 \t lr : 0.0005 \t training loss : 0.26605117321014404 \t time taken : 39.8 sec\n",
            "batch: 50 \t lr : 0.0005 \t training loss : 0.314774751663208 \t time taken : 40.0 sec\n",
            "batch: 60 \t lr : 0.0005 \t training loss : 0.30624717473983765 \t time taken : 43.3 sec\n",
            "batch: 70 \t lr : 0.0005 \t training loss : 0.29004207253456116 \t time taken : 39.6 sec\n",
            "batch: 80 \t lr : 0.0005 \t training loss : 0.24033156037330627 \t time taken : 40.1 sec\n",
            "batch: 90 \t lr : 0.0005 \t training loss : 0.27006423473358154 \t time taken : 40.5 sec\n",
            "batch: 100 \t lr : 0.0005 \t training loss : 0.24289122223854065 \t time taken : 40.3 sec\n",
            "batch: 110 \t lr : 0.0005 \t training loss : 0.23335659503936768 \t time taken : 39.5 sec\n",
            "batch: 120 \t lr : 0.0005 \t training loss : 0.22892065346240997 \t time taken : 38.8 sec\n",
            "batch: 130 \t lr : 0.0005 \t training loss : 0.2656633257865906 \t time taken : 38.9 sec\n",
            "batch: 140 \t lr : 0.0005 \t training loss : 0.2972329258918762 \t time taken : 45.1 sec\n",
            "batch: 150 \t lr : 0.0005 \t training loss : 0.25078046321868896 \t time taken : 40.8 sec\n",
            "batch: 160 \t lr : 0.0005 \t training loss : 0.22769403457641602 \t time taken : 43.8 sec\n",
            "batch: 170 \t lr : 0.0005 \t training loss : 0.25824183225631714 \t time taken : 39.2 sec\n",
            "batch: 180 \t lr : 0.0005 \t training loss : 0.24424539506435394 \t time taken : 42.6 sec\n",
            "batch: 190 \t lr : 0.0005 \t training loss : 0.306109756231308 \t time taken : 40.8 sec\n",
            "batch: 200 \t lr : 0.0005 \t training loss : 0.28159457445144653 \t time taken : 38.6 sec\n",
            "batch: 210 \t lr : 0.0005 \t training loss : 0.2409481257200241 \t time taken : 41.1 sec\n",
            "batch: 220 \t lr : 0.0005 \t training loss : 0.27586036920547485 \t time taken : 41.2 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 6 \tval_loss 0.0061859845437786795 \tdistance 12.18\n",
            "Best loss: 0.0061859845437786795, epoch: 6\n",
            "training loss: 0.0061859845437786795, epoch: 6\n",
            "Epoch completed: 43.0 min\n",
            "LR =  0.0005\n",
            "========================================\n",
            "Epoch:  7\n",
            "batch: 10 \t lr : 0.0005 \t training loss : 0.268604576587677 \t time taken : 47.4 sec\n",
            "batch: 20 \t lr : 0.0005 \t training loss : 0.2193470299243927 \t time taken : 38.0 sec\n",
            "batch: 30 \t lr : 0.0005 \t training loss : 0.19483979046344757 \t time taken : 41.2 sec\n",
            "batch: 40 \t lr : 0.0005 \t training loss : 0.25022125244140625 \t time taken : 39.3 sec\n",
            "batch: 50 \t lr : 0.0005 \t training loss : 0.24561621248722076 \t time taken : 43.3 sec\n",
            "batch: 60 \t lr : 0.0005 \t training loss : 0.23647603392601013 \t time taken : 39.9 sec\n",
            "batch: 70 \t lr : 0.0005 \t training loss : 0.22746948897838593 \t time taken : 40.9 sec\n",
            "batch: 80 \t lr : 0.0005 \t training loss : 0.27180981636047363 \t time taken : 44.0 sec\n",
            "batch: 90 \t lr : 0.0005 \t training loss : 0.22344668209552765 \t time taken : 40.5 sec\n",
            "batch: 100 \t lr : 0.0005 \t training loss : 0.2687990665435791 \t time taken : 41.5 sec\n",
            "batch: 110 \t lr : 0.0005 \t training loss : 0.2633129358291626 \t time taken : 44.4 sec\n",
            "batch: 120 \t lr : 0.0005 \t training loss : 0.2238650918006897 \t time taken : 38.6 sec\n",
            "batch: 130 \t lr : 0.0005 \t training loss : 0.267658531665802 \t time taken : 42.6 sec\n",
            "batch: 140 \t lr : 0.0005 \t training loss : 0.24811695516109467 \t time taken : 42.5 sec\n",
            "batch: 150 \t lr : 0.0005 \t training loss : 0.2587624490261078 \t time taken : 42.5 sec\n",
            "batch: 160 \t lr : 0.0005 \t training loss : 0.21428951621055603 \t time taken : 39.4 sec\n",
            "batch: 170 \t lr : 0.0005 \t training loss : 0.24206919968128204 \t time taken : 41.2 sec\n",
            "batch: 180 \t lr : 0.0005 \t training loss : 0.2820822596549988 \t time taken : 40.8 sec\n",
            "batch: 190 \t lr : 0.0005 \t training loss : 0.2213856726884842 \t time taken : 40.7 sec\n",
            "batch: 200 \t lr : 0.0005 \t training loss : 0.22444278001785278 \t time taken : 41.7 sec\n",
            "batch: 210 \t lr : 0.0005 \t training loss : 0.2612829804420471 \t time taken : 39.4 sec\n",
            "batch: 220 \t lr : 0.0005 \t training loss : 0.2277749478816986 \t time taken : 39.4 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 7 \tval_loss 0.006142315309156071 \tdistance 11.778181818181817\n",
            "Best loss: 0.006142315309156071, epoch: 7\n",
            "training loss: 0.006142315309156071, epoch: 7\n",
            "Epoch completed: 42.8 min\n",
            "LR =  0.0005\n",
            "========================================\n",
            "Epoch:  8\n",
            "batch: 10 \t lr : 0.0005 \t training loss : 0.26222920417785645 \t time taken : 45.1 sec\n",
            "batch: 20 \t lr : 0.0005 \t training loss : 0.25420454144477844 \t time taken : 41.1 sec\n",
            "batch: 30 \t lr : 0.0005 \t training loss : 0.23197384178638458 \t time taken : 41.0 sec\n",
            "batch: 40 \t lr : 0.0005 \t training loss : 0.2244645357131958 \t time taken : 40.1 sec\n",
            "batch: 50 \t lr : 0.0005 \t training loss : 0.23464787006378174 \t time taken : 41.4 sec\n",
            "batch: 60 \t lr : 0.0005 \t training loss : 0.2522498369216919 \t time taken : 39.6 sec\n",
            "batch: 70 \t lr : 0.0005 \t training loss : 0.2283829152584076 \t time taken : 42.6 sec\n",
            "batch: 80 \t lr : 0.0005 \t training loss : 0.24207532405853271 \t time taken : 41.6 sec\n",
            "batch: 90 \t lr : 0.0005 \t training loss : 0.24788278341293335 \t time taken : 39.2 sec\n",
            "batch: 100 \t lr : 0.0005 \t training loss : 0.26896125078201294 \t time taken : 41.0 sec\n",
            "batch: 110 \t lr : 0.0005 \t training loss : 0.2252027839422226 \t time taken : 40.6 sec\n",
            "batch: 120 \t lr : 0.0005 \t training loss : 0.2861401438713074 \t time taken : 42.3 sec\n",
            "batch: 130 \t lr : 0.0005 \t training loss : 0.2769694924354553 \t time taken : 44.6 sec\n",
            "batch: 140 \t lr : 0.0005 \t training loss : 0.25432080030441284 \t time taken : 41.8 sec\n",
            "batch: 150 \t lr : 0.0005 \t training loss : 0.24703402817249298 \t time taken : 41.2 sec\n",
            "batch: 160 \t lr : 0.0005 \t training loss : 0.282960444688797 \t time taken : 42.5 sec\n",
            "batch: 170 \t lr : 0.0005 \t training loss : 0.2595844864845276 \t time taken : 39.5 sec\n",
            "batch: 180 \t lr : 0.0005 \t training loss : 0.25908035039901733 \t time taken : 41.8 sec\n",
            "batch: 190 \t lr : 0.0005 \t training loss : 0.22994841635227203 \t time taken : 39.8 sec\n",
            "batch: 200 \t lr : 0.0005 \t training loss : 0.2520371973514557 \t time taken : 37.7 sec\n",
            "batch: 210 \t lr : 0.0005 \t training loss : 0.2279297560453415 \t time taken : 39.8 sec\n",
            "batch: 220 \t lr : 0.0005 \t training loss : 0.22234317660331726 \t time taken : 43.3 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 8 \tval_loss 0.0060543442856181755 \tdistance 11.680909090909092\n",
            "Best loss: 0.0060543442856181755, epoch: 8\n",
            "training loss: 0.0060543442856181755, epoch: 8\n",
            "Epoch completed: 43.0 min\n",
            "LR =  0.0005\n",
            "========================================\n",
            "Epoch:  9\n",
            "batch: 10 \t lr : 0.0005 \t training loss : 0.20188181102275848 \t time taken : 46.0 sec\n",
            "batch: 20 \t lr : 0.0005 \t training loss : 0.2068624049425125 \t time taken : 39.3 sec\n",
            "batch: 30 \t lr : 0.0005 \t training loss : 0.1786903738975525 \t time taken : 40.6 sec\n",
            "batch: 40 \t lr : 0.0005 \t training loss : 0.20298081636428833 \t time taken : 40.1 sec\n",
            "batch: 50 \t lr : 0.0005 \t training loss : 0.216597318649292 \t time taken : 39.6 sec\n",
            "batch: 60 \t lr : 0.0005 \t training loss : 0.20648124814033508 \t time taken : 39.7 sec\n",
            "batch: 70 \t lr : 0.0005 \t training loss : 0.20978084206581116 \t time taken : 46.0 sec\n",
            "batch: 80 \t lr : 0.0005 \t training loss : 0.20273995399475098 \t time taken : 42.0 sec\n",
            "batch: 90 \t lr : 0.0005 \t training loss : 0.23488092422485352 \t time taken : 41.1 sec\n",
            "batch: 100 \t lr : 0.0005 \t training loss : 0.23023366928100586 \t time taken : 42.7 sec\n",
            "batch: 110 \t lr : 0.0005 \t training loss : 0.234462171792984 \t time taken : 40.2 sec\n",
            "batch: 120 \t lr : 0.0005 \t training loss : 0.2491060048341751 \t time taken : 39.9 sec\n",
            "batch: 130 \t lr : 0.0005 \t training loss : 0.2607044577598572 \t time taken : 40.9 sec\n",
            "batch: 140 \t lr : 0.0005 \t training loss : 0.23263755440711975 \t time taken : 39.0 sec\n",
            "batch: 150 \t lr : 0.0005 \t training loss : 0.2595903277397156 \t time taken : 42.9 sec\n",
            "batch: 160 \t lr : 0.0005 \t training loss : 0.2389514446258545 \t time taken : 39.9 sec\n",
            "batch: 170 \t lr : 0.0005 \t training loss : 0.2853507399559021 \t time taken : 40.5 sec\n",
            "batch: 180 \t lr : 0.0005 \t training loss : 0.23966634273529053 \t time taken : 39.6 sec\n",
            "batch: 190 \t lr : 0.0005 \t training loss : 0.283505916595459 \t time taken : 40.1 sec\n",
            "batch: 200 \t lr : 0.0005 \t training loss : 0.3176475763320923 \t time taken : 42.7 sec\n",
            "batch: 210 \t lr : 0.0005 \t training loss : 0.2921682894229889 \t time taken : 38.8 sec\n",
            "batch: 220 \t lr : 0.0005 \t training loss : 0.25504177808761597 \t time taken : 41.3 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 9 \tval_loss 0.006060320369221947 \tdistance 11.61909090909091\n",
            "Epoch completed: 42.7 min\n",
            "LR =  0.0005\n",
            "========================================\n",
            "Epoch:  10\n",
            "batch: 10 \t lr : 0.0005 \t training loss : 0.22886300086975098 \t time taken : 46.3 sec\n",
            "batch: 20 \t lr : 0.0005 \t training loss : 0.2529200315475464 \t time taken : 40.2 sec\n",
            "batch: 30 \t lr : 0.0005 \t training loss : 0.23177988827228546 \t time taken : 40.5 sec\n",
            "batch: 40 \t lr : 0.0005 \t training loss : 0.24966908991336823 \t time taken : 40.4 sec\n",
            "batch: 50 \t lr : 0.0005 \t training loss : 0.2132578194141388 \t time taken : 39.2 sec\n",
            "batch: 60 \t lr : 0.0005 \t training loss : 0.19085422158241272 \t time taken : 41.2 sec\n",
            "batch: 70 \t lr : 0.0005 \t training loss : 0.2435142993927002 \t time taken : 42.8 sec\n",
            "batch: 80 \t lr : 0.0005 \t training loss : 0.23568354547023773 \t time taken : 39.5 sec\n",
            "batch: 90 \t lr : 0.0005 \t training loss : 0.20099636912345886 \t time taken : 41.8 sec\n",
            "batch: 100 \t lr : 0.0005 \t training loss : 0.20196625590324402 \t time taken : 40.5 sec\n",
            "batch: 110 \t lr : 0.0005 \t training loss : 0.22393165528774261 \t time taken : 39.7 sec\n",
            "batch: 120 \t lr : 0.0005 \t training loss : 0.23875311017036438 \t time taken : 41.6 sec\n",
            "batch: 130 \t lr : 0.0005 \t training loss : 0.20085319876670837 \t time taken : 41.1 sec\n",
            "batch: 140 \t lr : 0.0005 \t training loss : 0.21336504817008972 \t time taken : 42.6 sec\n",
            "batch: 150 \t lr : 0.0005 \t training loss : 0.20874878764152527 \t time taken : 43.5 sec\n",
            "batch: 160 \t lr : 0.0005 \t training loss : 0.21302367746829987 \t time taken : 41.4 sec\n",
            "batch: 170 \t lr : 0.0005 \t training loss : 0.22365859150886536 \t time taken : 40.7 sec\n",
            "batch: 180 \t lr : 0.0005 \t training loss : 0.21670496463775635 \t time taken : 41.8 sec\n",
            "batch: 190 \t lr : 0.0005 \t training loss : 0.26206064224243164 \t time taken : 40.6 sec\n",
            "batch: 200 \t lr : 0.0005 \t training loss : 0.20658466219902039 \t time taken : 41.5 sec\n",
            "batch: 210 \t lr : 0.0005 \t training loss : 0.22214928269386292 \t time taken : 39.4 sec\n",
            "batch: 220 \t lr : 0.0005 \t training loss : 0.20083311200141907 \t time taken : 41.0 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 10 \tval_loss 0.006244257512417707 \tdistance 11.885454545454545\n",
            "Epoch completed: 43.0 min\n",
            "LR =  0.0005\n",
            "========================================\n",
            "Epoch:  11\n",
            "batch: 10 \t lr : 0.0005 \t training loss : 0.253062903881073 \t time taken : 49.5 sec\n",
            "batch: 20 \t lr : 0.0005 \t training loss : 0.211698517203331 \t time taken : 41.4 sec\n",
            "batch: 30 \t lr : 0.0005 \t training loss : 0.22172267735004425 \t time taken : 40.1 sec\n",
            "batch: 40 \t lr : 0.0005 \t training loss : 0.19314920902252197 \t time taken : 38.0 sec\n",
            "batch: 50 \t lr : 0.0005 \t training loss : 0.21367928385734558 \t time taken : 45.0 sec\n",
            "batch: 60 \t lr : 0.0005 \t training loss : 0.2022208869457245 \t time taken : 40.6 sec\n",
            "batch: 70 \t lr : 0.0005 \t training loss : 0.22881297767162323 \t time taken : 38.3 sec\n",
            "batch: 80 \t lr : 0.0005 \t training loss : 0.20003482699394226 \t time taken : 41.0 sec\n",
            "batch: 90 \t lr : 0.0005 \t training loss : 0.24587728083133698 \t time taken : 40.0 sec\n",
            "batch: 100 \t lr : 0.0005 \t training loss : 0.26546746492385864 \t time taken : 43.0 sec\n",
            "batch: 110 \t lr : 0.0005 \t training loss : 0.23959732055664062 \t time taken : 41.3 sec\n",
            "batch: 120 \t lr : 0.0005 \t training loss : 0.22460100054740906 \t time taken : 40.9 sec\n",
            "batch: 130 \t lr : 0.0005 \t training loss : 0.2247842401266098 \t time taken : 40.1 sec\n",
            "batch: 140 \t lr : 0.0005 \t training loss : 0.20069298148155212 \t time taken : 43.7 sec\n",
            "batch: 150 \t lr : 0.0005 \t training loss : 0.2150411754846573 \t time taken : 39.8 sec\n",
            "batch: 160 \t lr : 0.0005 \t training loss : 0.26900410652160645 \t time taken : 41.7 sec\n",
            "batch: 170 \t lr : 0.0005 \t training loss : 0.22864210605621338 \t time taken : 39.0 sec\n",
            "batch: 180 \t lr : 0.0005 \t training loss : 0.21481910347938538 \t time taken : 41.6 sec\n",
            "batch: 190 \t lr : 0.0005 \t training loss : 0.21330003440380096 \t time taken : 38.6 sec\n",
            "batch: 200 \t lr : 0.0005 \t training loss : 0.20817622542381287 \t time taken : 41.8 sec\n",
            "batch: 210 \t lr : 0.0005 \t training loss : 0.21130380034446716 \t time taken : 39.9 sec\n",
            "batch: 220 \t lr : 0.0005 \t training loss : 0.2037295252084732 \t time taken : 39.0 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 11 \tval_loss 0.006058101044459777 \tdistance 11.511818181818182\n",
            "Epoch    11: reducing learning rate of group 0 to 4.0000e-04.\n",
            "Epoch completed: 42.8 min\n",
            "LR =  0.0004\n",
            "========================================\n",
            "Epoch:  12\n",
            "batch: 10 \t lr : 0.0004 \t training loss : 0.20580708980560303 \t time taken : 46.4 sec\n",
            "batch: 20 \t lr : 0.0004 \t training loss : 0.1866917610168457 \t time taken : 40.1 sec\n",
            "batch: 30 \t lr : 0.0004 \t training loss : 0.15952207148075104 \t time taken : 41.9 sec\n",
            "batch: 40 \t lr : 0.0004 \t training loss : 0.17769362032413483 \t time taken : 39.2 sec\n",
            "batch: 50 \t lr : 0.0004 \t training loss : 0.1985044628381729 \t time taken : 40.3 sec\n",
            "batch: 60 \t lr : 0.0004 \t training loss : 0.18660874664783478 \t time taken : 41.8 sec\n",
            "batch: 70 \t lr : 0.0004 \t training loss : 0.1939120590686798 \t time taken : 40.3 sec\n",
            "batch: 80 \t lr : 0.0004 \t training loss : 0.2223786562681198 \t time taken : 40.6 sec\n",
            "batch: 90 \t lr : 0.0004 \t training loss : 0.16318240761756897 \t time taken : 39.4 sec\n",
            "batch: 100 \t lr : 0.0004 \t training loss : 0.1737818568944931 \t time taken : 40.6 sec\n",
            "batch: 110 \t lr : 0.0004 \t training loss : 0.18866610527038574 \t time taken : 40.4 sec\n",
            "batch: 120 \t lr : 0.0004 \t training loss : 0.17486155033111572 \t time taken : 40.1 sec\n",
            "batch: 130 \t lr : 0.0004 \t training loss : 0.1772000640630722 \t time taken : 40.0 sec\n",
            "batch: 140 \t lr : 0.0004 \t training loss : 0.19436509907245636 \t time taken : 40.8 sec\n",
            "batch: 150 \t lr : 0.0004 \t training loss : 0.16912876069545746 \t time taken : 41.6 sec\n",
            "batch: 160 \t lr : 0.0004 \t training loss : 0.20695963501930237 \t time taken : 40.9 sec\n",
            "batch: 170 \t lr : 0.0004 \t training loss : 0.17445723712444305 \t time taken : 39.6 sec\n",
            "batch: 180 \t lr : 0.0004 \t training loss : 0.17030364274978638 \t time taken : 43.1 sec\n",
            "batch: 190 \t lr : 0.0004 \t training loss : 0.20355841517448425 \t time taken : 40.6 sec\n",
            "batch: 200 \t lr : 0.0004 \t training loss : 0.1869397610425949 \t time taken : 39.8 sec\n",
            "batch: 210 \t lr : 0.0004 \t training loss : 0.1889701783657074 \t time taken : 41.7 sec\n",
            "batch: 220 \t lr : 0.0004 \t training loss : 0.18265219032764435 \t time taken : 42.9 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 12 \tval_loss 0.0058266391537406224 \tdistance 10.910454545454545\n",
            "Best loss: 0.0058266391537406224, epoch: 12\n",
            "training loss: 0.0058266391537406224, epoch: 12\n",
            "Epoch completed: 42.8 min\n",
            "LR =  0.0004\n",
            "========================================\n",
            "Epoch:  13\n",
            "batch: 10 \t lr : 0.0004 \t training loss : 0.17912504076957703 \t time taken : 45.8 sec\n",
            "batch: 20 \t lr : 0.0004 \t training loss : 0.20207110047340393 \t time taken : 41.4 sec\n",
            "batch: 30 \t lr : 0.0004 \t training loss : 0.15478232502937317 \t time taken : 39.7 sec\n",
            "batch: 40 \t lr : 0.0004 \t training loss : 0.16065235435962677 \t time taken : 40.2 sec\n",
            "batch: 50 \t lr : 0.0004 \t training loss : 0.17599201202392578 \t time taken : 41.7 sec\n",
            "batch: 60 \t lr : 0.0004 \t training loss : 0.19486676156520844 \t time taken : 39.8 sec\n",
            "batch: 70 \t lr : 0.0004 \t training loss : 0.17437762022018433 \t time taken : 40.1 sec\n",
            "batch: 80 \t lr : 0.0004 \t training loss : 0.1677122414112091 \t time taken : 39.9 sec\n",
            "batch: 90 \t lr : 0.0004 \t training loss : 0.18014337122440338 \t time taken : 41.1 sec\n",
            "batch: 100 \t lr : 0.0004 \t training loss : 0.16001740097999573 \t time taken : 40.9 sec\n",
            "batch: 110 \t lr : 0.0004 \t training loss : 0.19742035865783691 \t time taken : 42.4 sec\n",
            "batch: 120 \t lr : 0.0004 \t training loss : 0.1701451539993286 \t time taken : 41.2 sec\n",
            "batch: 130 \t lr : 0.0004 \t training loss : 0.18962320685386658 \t time taken : 40.7 sec\n",
            "batch: 140 \t lr : 0.0004 \t training loss : 0.23991583287715912 \t time taken : 41.5 sec\n",
            "batch: 150 \t lr : 0.0004 \t training loss : 0.22083422541618347 \t time taken : 40.4 sec\n",
            "batch: 160 \t lr : 0.0004 \t training loss : 0.2174784541130066 \t time taken : 41.1 sec\n",
            "batch: 170 \t lr : 0.0004 \t training loss : 0.21092481911182404 \t time taken : 41.6 sec\n",
            "batch: 180 \t lr : 0.0004 \t training loss : 0.2289779782295227 \t time taken : 43.4 sec\n",
            "batch: 190 \t lr : 0.0004 \t training loss : 0.2344943881034851 \t time taken : 38.8 sec\n",
            "batch: 200 \t lr : 0.0004 \t training loss : 0.21389108896255493 \t time taken : 41.6 sec\n",
            "batch: 210 \t lr : 0.0004 \t training loss : 0.22366273403167725 \t time taken : 42.9 sec\n",
            "batch: 220 \t lr : 0.0004 \t training loss : 0.21952588856220245 \t time taken : 43.6 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 13 \tval_loss 0.006006033258004622 \tdistance 11.236818181818181\n",
            "Epoch completed: 42.7 min\n",
            "LR =  0.0004\n",
            "========================================\n",
            "Epoch:  14\n",
            "batch: 10 \t lr : 0.0004 \t training loss : 0.20605683326721191 \t time taken : 48.4 sec\n",
            "batch: 20 \t lr : 0.0004 \t training loss : 0.18300658464431763 \t time taken : 41.5 sec\n",
            "batch: 30 \t lr : 0.0004 \t training loss : 0.1790197789669037 \t time taken : 38.2 sec\n",
            "batch: 40 \t lr : 0.0004 \t training loss : 0.19003786146640778 \t time taken : 41.1 sec\n",
            "batch: 50 \t lr : 0.0004 \t training loss : 0.1796225905418396 \t time taken : 42.5 sec\n",
            "batch: 60 \t lr : 0.0004 \t training loss : 0.19924360513687134 \t time taken : 39.2 sec\n",
            "batch: 70 \t lr : 0.0004 \t training loss : 0.18209412693977356 \t time taken : 41.2 sec\n",
            "batch: 80 \t lr : 0.0004 \t training loss : 0.17265668511390686 \t time taken : 41.7 sec\n",
            "batch: 90 \t lr : 0.0004 \t training loss : 0.19757243990898132 \t time taken : 42.9 sec\n",
            "batch: 100 \t lr : 0.0004 \t training loss : 0.23959369957447052 \t time taken : 40.0 sec\n",
            "batch: 110 \t lr : 0.0004 \t training loss : 0.22398161888122559 \t time taken : 42.6 sec\n",
            "batch: 120 \t lr : 0.0004 \t training loss : 0.2336069941520691 \t time taken : 40.2 sec\n",
            "batch: 130 \t lr : 0.0004 \t training loss : 0.19508063793182373 \t time taken : 41.6 sec\n",
            "batch: 140 \t lr : 0.0004 \t training loss : 0.22489729523658752 \t time taken : 44.3 sec\n",
            "batch: 150 \t lr : 0.0004 \t training loss : 0.18787291646003723 \t time taken : 38.6 sec\n",
            "batch: 160 \t lr : 0.0004 \t training loss : 0.17263174057006836 \t time taken : 41.2 sec\n",
            "batch: 170 \t lr : 0.0004 \t training loss : 0.18083804845809937 \t time taken : 39.9 sec\n",
            "batch: 180 \t lr : 0.0004 \t training loss : 0.20710302889347076 \t time taken : 39.9 sec\n",
            "batch: 190 \t lr : 0.0004 \t training loss : 0.18603649735450745 \t time taken : 39.5 sec\n",
            "batch: 200 \t lr : 0.0004 \t training loss : 0.2121691256761551 \t time taken : 40.9 sec\n",
            "batch: 210 \t lr : 0.0004 \t training loss : 0.17950667440891266 \t time taken : 41.9 sec\n",
            "batch: 220 \t lr : 0.0004 \t training loss : 0.21771499514579773 \t time taken : 40.4 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 14 \tval_loss 0.005973997955972498 \tdistance 11.029545454545454\n",
            "Epoch completed: 43.1 min\n",
            "LR =  0.0004\n",
            "========================================\n",
            "Epoch:  15\n",
            "batch: 10 \t lr : 0.0004 \t training loss : 0.18280786275863647 \t time taken : 46.5 sec\n",
            "batch: 20 \t lr : 0.0004 \t training loss : 0.16724690794944763 \t time taken : 42.7 sec\n",
            "batch: 30 \t lr : 0.0004 \t training loss : 0.17906323075294495 \t time taken : 42.0 sec\n",
            "batch: 40 \t lr : 0.0004 \t training loss : 0.16725574433803558 \t time taken : 41.6 sec\n",
            "batch: 50 \t lr : 0.0004 \t training loss : 0.17636406421661377 \t time taken : 41.0 sec\n",
            "batch: 60 \t lr : 0.0004 \t training loss : 0.17492593824863434 \t time taken : 40.9 sec\n",
            "batch: 70 \t lr : 0.0004 \t training loss : 0.15642158687114716 \t time taken : 41.4 sec\n",
            "batch: 80 \t lr : 0.0004 \t training loss : 0.17836478352546692 \t time taken : 39.1 sec\n",
            "batch: 90 \t lr : 0.0004 \t training loss : 0.1742573082447052 \t time taken : 40.0 sec\n",
            "batch: 100 \t lr : 0.0004 \t training loss : 0.1700231283903122 \t time taken : 40.3 sec\n",
            "batch: 110 \t lr : 0.0004 \t training loss : 0.1784401834011078 \t time taken : 42.8 sec\n",
            "batch: 120 \t lr : 0.0004 \t training loss : 0.21754050254821777 \t time taken : 39.5 sec\n",
            "batch: 130 \t lr : 0.0004 \t training loss : 0.20962558686733246 \t time taken : 42.7 sec\n",
            "batch: 140 \t lr : 0.0004 \t training loss : 0.20871546864509583 \t time taken : 40.8 sec\n",
            "batch: 150 \t lr : 0.0004 \t training loss : 0.18158221244812012 \t time taken : 39.3 sec\n",
            "batch: 160 \t lr : 0.0004 \t training loss : 0.16877365112304688 \t time taken : 41.4 sec\n",
            "batch: 170 \t lr : 0.0004 \t training loss : 0.19715480506420135 \t time taken : 41.7 sec\n",
            "batch: 180 \t lr : 0.0004 \t training loss : 0.18105724453926086 \t time taken : 38.4 sec\n",
            "batch: 190 \t lr : 0.0004 \t training loss : 0.15144819021224976 \t time taken : 41.1 sec\n",
            "batch: 200 \t lr : 0.0004 \t training loss : 0.20637233555316925 \t time taken : 41.0 sec\n",
            "batch: 210 \t lr : 0.0004 \t training loss : 0.18124724924564362 \t time taken : 41.2 sec\n",
            "batch: 220 \t lr : 0.0004 \t training loss : 0.18954047560691833 \t time taken : 42.3 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 15 \tval_loss 0.0058513438701629636 \tdistance 10.652727272727272\n",
            "Epoch    15: reducing learning rate of group 0 to 3.2000e-04.\n",
            "Epoch completed: 43.7 min\n",
            "LR =  0.00032\n",
            "========================================\n",
            "Epoch:  16\n",
            "batch: 10 \t lr : 0.00032 \t training loss : 0.18474383652210236 \t time taken : 44.9 sec\n",
            "batch: 20 \t lr : 0.00032 \t training loss : 0.15080957114696503 \t time taken : 38.2 sec\n",
            "batch: 30 \t lr : 0.00032 \t training loss : 0.16809964179992676 \t time taken : 40.4 sec\n",
            "batch: 40 \t lr : 0.00032 \t training loss : 0.15852594375610352 \t time taken : 41.7 sec\n",
            "batch: 50 \t lr : 0.00032 \t training loss : 0.15630969405174255 \t time taken : 41.9 sec\n",
            "batch: 60 \t lr : 0.00032 \t training loss : 0.1566106677055359 \t time taken : 42.3 sec\n",
            "batch: 70 \t lr : 0.00032 \t training loss : 0.1659049093723297 \t time taken : 42.9 sec\n",
            "batch: 80 \t lr : 0.00032 \t training loss : 0.1510026752948761 \t time taken : 42.8 sec\n",
            "batch: 90 \t lr : 0.00032 \t training loss : 0.15707546472549438 \t time taken : 39.5 sec\n",
            "batch: 100 \t lr : 0.00032 \t training loss : 0.141826793551445 \t time taken : 40.7 sec\n",
            "batch: 110 \t lr : 0.00032 \t training loss : 0.15152215957641602 \t time taken : 40.8 sec\n",
            "batch: 120 \t lr : 0.00032 \t training loss : 0.16203641891479492 \t time taken : 39.2 sec\n",
            "batch: 130 \t lr : 0.00032 \t training loss : 0.13042038679122925 \t time taken : 43.0 sec\n",
            "batch: 140 \t lr : 0.00032 \t training loss : 0.15593448281288147 \t time taken : 42.5 sec\n",
            "batch: 150 \t lr : 0.00032 \t training loss : 0.14182807505130768 \t time taken : 40.7 sec\n",
            "batch: 160 \t lr : 0.00032 \t training loss : 0.1412140130996704 \t time taken : 42.2 sec\n",
            "batch: 170 \t lr : 0.00032 \t training loss : 0.15190553665161133 \t time taken : 40.8 sec\n",
            "batch: 180 \t lr : 0.00032 \t training loss : 0.15485943853855133 \t time taken : 40.1 sec\n",
            "batch: 190 \t lr : 0.00032 \t training loss : 0.1281406134366989 \t time taken : 38.7 sec\n",
            "batch: 200 \t lr : 0.00032 \t training loss : 0.16362182796001434 \t time taken : 40.8 sec\n",
            "batch: 210 \t lr : 0.00032 \t training loss : 0.14131329953670502 \t time taken : 41.6 sec\n",
            "batch: 220 \t lr : 0.00032 \t training loss : 0.17092391848564148 \t time taken : 39.8 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 16 \tval_loss 0.0057112990184263746 \tdistance 10.251818181818182\n",
            "Best loss: 0.0057112990184263746, epoch: 16\n",
            "training loss: 0.0057112990184263746, epoch: 16\n",
            "Epoch completed: 43.7 min\n",
            "LR =  0.00032\n",
            "========================================\n",
            "Epoch:  17\n",
            "batch: 10 \t lr : 0.00032 \t training loss : 0.135835200548172 \t time taken : 47.5 sec\n",
            "batch: 20 \t lr : 0.00032 \t training loss : 0.13804470002651215 \t time taken : 40.8 sec\n",
            "batch: 30 \t lr : 0.00032 \t training loss : 0.149127796292305 \t time taken : 41.1 sec\n",
            "batch: 40 \t lr : 0.00032 \t training loss : 0.1764068305492401 \t time taken : 39.7 sec\n",
            "batch: 50 \t lr : 0.00032 \t training loss : 0.1497468203306198 \t time taken : 41.9 sec\n",
            "batch: 60 \t lr : 0.00032 \t training loss : 0.12159593403339386 \t time taken : 38.6 sec\n",
            "batch: 70 \t lr : 0.00032 \t training loss : 0.149186372756958 \t time taken : 42.1 sec\n",
            "batch: 80 \t lr : 0.00032 \t training loss : 0.13966938853263855 \t time taken : 39.3 sec\n",
            "batch: 90 \t lr : 0.00032 \t training loss : 0.14468863606452942 \t time taken : 40.4 sec\n",
            "batch: 100 \t lr : 0.00032 \t training loss : 0.13597533106803894 \t time taken : 40.4 sec\n",
            "batch: 110 \t lr : 0.00032 \t training loss : 0.14787399768829346 \t time taken : 40.7 sec\n",
            "batch: 120 \t lr : 0.00032 \t training loss : 0.1315271556377411 \t time taken : 39.7 sec\n",
            "batch: 130 \t lr : 0.00032 \t training loss : 0.15882709622383118 \t time taken : 41.0 sec\n",
            "batch: 140 \t lr : 0.00032 \t training loss : 0.14553332328796387 \t time taken : 40.0 sec\n",
            "batch: 150 \t lr : 0.00032 \t training loss : 0.14367009699344635 \t time taken : 41.1 sec\n",
            "batch: 160 \t lr : 0.00032 \t training loss : 0.1746707707643509 \t time taken : 42.6 sec\n",
            "batch: 170 \t lr : 0.00032 \t training loss : 0.13833385705947876 \t time taken : 42.0 sec\n",
            "batch: 180 \t lr : 0.00032 \t training loss : 0.16986674070358276 \t time taken : 40.8 sec\n",
            "batch: 190 \t lr : 0.00032 \t training loss : 0.17022091150283813 \t time taken : 40.4 sec\n",
            "batch: 200 \t lr : 0.00032 \t training loss : 0.13273465633392334 \t time taken : 41.0 sec\n",
            "batch: 210 \t lr : 0.00032 \t training loss : 0.15855863690376282 \t time taken : 39.3 sec\n",
            "batch: 220 \t lr : 0.00032 \t training loss : 0.15802446007728577 \t time taken : 43.9 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 17 \tval_loss 0.005756098912520842 \tdistance 10.392727272727273\n",
            "Epoch completed: 43.2 min\n",
            "LR =  0.00032\n",
            "========================================\n",
            "Epoch:  18\n",
            "batch: 10 \t lr : 0.00032 \t training loss : 0.15546008944511414 \t time taken : 46.2 sec\n",
            "batch: 20 \t lr : 0.00032 \t training loss : 0.1368362009525299 \t time taken : 41.5 sec\n",
            "batch: 30 \t lr : 0.00032 \t training loss : 0.13945314288139343 \t time taken : 40.4 sec\n",
            "batch: 40 \t lr : 0.00032 \t training loss : 0.18098218739032745 \t time taken : 41.5 sec\n",
            "batch: 50 \t lr : 0.00032 \t training loss : 0.17848265171051025 \t time taken : 41.8 sec\n",
            "batch: 60 \t lr : 0.00032 \t training loss : 0.13769738376140594 \t time taken : 41.4 sec\n",
            "batch: 70 \t lr : 0.00032 \t training loss : 0.1483200192451477 \t time taken : 40.3 sec\n",
            "batch: 80 \t lr : 0.00032 \t training loss : 0.16619645059108734 \t time taken : 41.5 sec\n",
            "batch: 90 \t lr : 0.00032 \t training loss : 0.1697414666414261 \t time taken : 40.8 sec\n",
            "batch: 100 \t lr : 0.00032 \t training loss : 0.20889289677143097 \t time taken : 41.1 sec\n",
            "batch: 110 \t lr : 0.00032 \t training loss : 0.20723149180412292 \t time taken : 43.3 sec\n",
            "batch: 120 \t lr : 0.00032 \t training loss : 0.19518610835075378 \t time taken : 41.1 sec\n",
            "batch: 130 \t lr : 0.00032 \t training loss : 0.188306525349617 \t time taken : 41.2 sec\n",
            "batch: 140 \t lr : 0.00032 \t training loss : 0.15862035751342773 \t time taken : 42.7 sec\n",
            "batch: 150 \t lr : 0.00032 \t training loss : 0.21301159262657166 \t time taken : 38.9 sec\n",
            "batch: 160 \t lr : 0.00032 \t training loss : 0.16439449787139893 \t time taken : 40.5 sec\n",
            "batch: 170 \t lr : 0.00032 \t training loss : 0.18529698252677917 \t time taken : 39.4 sec\n",
            "batch: 180 \t lr : 0.00032 \t training loss : 0.18746134638786316 \t time taken : 41.0 sec\n",
            "batch: 190 \t lr : 0.00032 \t training loss : 0.16079455614089966 \t time taken : 38.7 sec\n",
            "batch: 200 \t lr : 0.00032 \t training loss : 0.18219345808029175 \t time taken : 39.7 sec\n",
            "batch: 210 \t lr : 0.00032 \t training loss : 0.171095609664917 \t time taken : 41.1 sec\n",
            "batch: 220 \t lr : 0.00032 \t training loss : 0.1697314977645874 \t time taken : 39.1 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 18 \tval_loss 0.005950169319456274 \tdistance 10.537272727272727\n",
            "Epoch completed: 43.6 min\n",
            "LR =  0.00032\n",
            "========================================\n",
            "Epoch:  19\n",
            "batch: 10 \t lr : 0.00032 \t training loss : 0.13280236721038818 \t time taken : 45.1 sec\n",
            "batch: 20 \t lr : 0.00032 \t training loss : 0.15806400775909424 \t time taken : 40.0 sec\n",
            "batch: 30 \t lr : 0.00032 \t training loss : 0.1573895812034607 \t time taken : 39.3 sec\n",
            "batch: 40 \t lr : 0.00032 \t training loss : 0.17143218219280243 \t time taken : 40.5 sec\n",
            "batch: 50 \t lr : 0.00032 \t training loss : 0.13568836450576782 \t time taken : 42.1 sec\n",
            "batch: 60 \t lr : 0.00032 \t training loss : 0.14525094628334045 \t time taken : 39.6 sec\n",
            "batch: 70 \t lr : 0.00032 \t training loss : 0.14693769812583923 \t time taken : 39.6 sec\n",
            "batch: 80 \t lr : 0.00032 \t training loss : 0.14150816202163696 \t time taken : 42.7 sec\n",
            "batch: 90 \t lr : 0.00032 \t training loss : 0.15473315119743347 \t time taken : 41.5 sec\n",
            "batch: 100 \t lr : 0.00032 \t training loss : 0.16112388670444489 \t time taken : 41.1 sec\n",
            "batch: 110 \t lr : 0.00032 \t training loss : 0.15647338330745697 \t time taken : 39.9 sec\n",
            "batch: 120 \t lr : 0.00032 \t training loss : 0.14694933593273163 \t time taken : 42.6 sec\n",
            "batch: 130 \t lr : 0.00032 \t training loss : 0.14524997770786285 \t time taken : 39.4 sec\n",
            "batch: 140 \t lr : 0.00032 \t training loss : 0.16135458648204803 \t time taken : 40.8 sec\n",
            "batch: 150 \t lr : 0.00032 \t training loss : 0.16939200460910797 \t time taken : 41.5 sec\n",
            "batch: 160 \t lr : 0.00032 \t training loss : 0.14782606065273285 \t time taken : 40.2 sec\n",
            "batch: 170 \t lr : 0.00032 \t training loss : 0.13296957314014435 \t time taken : 39.0 sec\n",
            "batch: 180 \t lr : 0.00032 \t training loss : 0.14630432426929474 \t time taken : 40.4 sec\n",
            "batch: 190 \t lr : 0.00032 \t training loss : 0.14815285801887512 \t time taken : 40.6 sec\n",
            "batch: 200 \t lr : 0.00032 \t training loss : 0.15087191760540009 \t time taken : 40.4 sec\n",
            "batch: 210 \t lr : 0.00032 \t training loss : 0.13044901192188263 \t time taken : 39.5 sec\n",
            "batch: 220 \t lr : 0.00032 \t training loss : 0.17769919335842133 \t time taken : 41.8 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 19 \tval_loss 0.005823611224239523 \tdistance 10.18\n",
            "Epoch    19: reducing learning rate of group 0 to 2.5600e-04.\n",
            "Epoch completed: 43.2 min\n",
            "LR =  0.00025600000000000004\n",
            "========================================\n",
            "Epoch:  20\n",
            "batch: 10 \t lr : 0.00025600000000000004 \t training loss : 0.11883188784122467 \t time taken : 47.8 sec\n",
            "batch: 20 \t lr : 0.00025600000000000004 \t training loss : 0.12413245439529419 \t time taken : 40.3 sec\n",
            "batch: 30 \t lr : 0.00025600000000000004 \t training loss : 0.12758971750736237 \t time taken : 41.5 sec\n",
            "batch: 40 \t lr : 0.00025600000000000004 \t training loss : 0.1779089868068695 \t time taken : 42.5 sec\n",
            "batch: 50 \t lr : 0.00025600000000000004 \t training loss : 0.1656932234764099 \t time taken : 39.3 sec\n",
            "batch: 60 \t lr : 0.00025600000000000004 \t training loss : 0.1643035113811493 \t time taken : 40.9 sec\n",
            "batch: 70 \t lr : 0.00025600000000000004 \t training loss : 0.14631234109401703 \t time taken : 40.1 sec\n",
            "batch: 80 \t lr : 0.00025600000000000004 \t training loss : 0.12855444848537445 \t time taken : 39.4 sec\n",
            "batch: 90 \t lr : 0.00025600000000000004 \t training loss : 0.12464592605829239 \t time taken : 40.2 sec\n",
            "batch: 100 \t lr : 0.00025600000000000004 \t training loss : 0.17003728449344635 \t time taken : 42.6 sec\n",
            "batch: 110 \t lr : 0.00025600000000000004 \t training loss : 0.14035838842391968 \t time taken : 42.6 sec\n",
            "batch: 120 \t lr : 0.00025600000000000004 \t training loss : 0.1179644763469696 \t time taken : 41.0 sec\n",
            "batch: 130 \t lr : 0.00025600000000000004 \t training loss : 0.13364312052726746 \t time taken : 39.7 sec\n",
            "batch: 140 \t lr : 0.00025600000000000004 \t training loss : 0.1564542055130005 \t time taken : 41.5 sec\n",
            "batch: 150 \t lr : 0.00025600000000000004 \t training loss : 0.15099406242370605 \t time taken : 38.9 sec\n",
            "batch: 160 \t lr : 0.00025600000000000004 \t training loss : 0.13999927043914795 \t time taken : 41.7 sec\n",
            "batch: 170 \t lr : 0.00025600000000000004 \t training loss : 0.1308550089597702 \t time taken : 42.3 sec\n",
            "batch: 180 \t lr : 0.00025600000000000004 \t training loss : 0.12400279939174652 \t time taken : 39.8 sec\n",
            "batch: 190 \t lr : 0.00025600000000000004 \t training loss : 0.14867126941680908 \t time taken : 39.5 sec\n",
            "batch: 200 \t lr : 0.00025600000000000004 \t training loss : 0.14701473712921143 \t time taken : 40.8 sec\n",
            "batch: 210 \t lr : 0.00025600000000000004 \t training loss : 0.14869199693202972 \t time taken : 40.7 sec\n",
            "batch: 220 \t lr : 0.00025600000000000004 \t training loss : 0.2056560516357422 \t time taken : 41.3 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 20 \tval_loss 0.006108536503531716 \tdistance 10.668181818181818\n",
            "Epoch completed: 43.6 min\n",
            "LR =  0.00025600000000000004\n",
            "========================================\n",
            "Epoch:  21\n",
            "batch: 10 \t lr : 0.00025600000000000004 \t training loss : 0.15871396660804749 \t time taken : 43.9 sec\n",
            "batch: 20 \t lr : 0.00025600000000000004 \t training loss : 0.16718947887420654 \t time taken : 42.7 sec\n",
            "batch: 30 \t lr : 0.00025600000000000004 \t training loss : 0.16361607611179352 \t time taken : 38.5 sec\n",
            "batch: 40 \t lr : 0.00025600000000000004 \t training loss : 0.15511535108089447 \t time taken : 40.5 sec\n",
            "batch: 50 \t lr : 0.00025600000000000004 \t training loss : 0.14221763610839844 \t time taken : 39.6 sec\n",
            "batch: 60 \t lr : 0.00025600000000000004 \t training loss : 0.15595117211341858 \t time taken : 40.7 sec\n",
            "batch: 70 \t lr : 0.00025600000000000004 \t training loss : 0.11942436546087265 \t time taken : 40.3 sec\n",
            "batch: 80 \t lr : 0.00025600000000000004 \t training loss : 0.14450499415397644 \t time taken : 42.0 sec\n",
            "batch: 90 \t lr : 0.00025600000000000004 \t training loss : 0.11757349967956543 \t time taken : 39.4 sec\n",
            "batch: 100 \t lr : 0.00025600000000000004 \t training loss : 0.12110245227813721 \t time taken : 40.7 sec\n",
            "batch: 110 \t lr : 0.00025600000000000004 \t training loss : 0.13740164041519165 \t time taken : 39.1 sec\n",
            "batch: 120 \t lr : 0.00025600000000000004 \t training loss : 0.12760473787784576 \t time taken : 42.6 sec\n",
            "batch: 130 \t lr : 0.00025600000000000004 \t training loss : 0.1341167390346527 \t time taken : 39.9 sec\n",
            "batch: 140 \t lr : 0.00025600000000000004 \t training loss : 0.1305117905139923 \t time taken : 40.0 sec\n",
            "batch: 150 \t lr : 0.00025600000000000004 \t training loss : 0.11862526834011078 \t time taken : 41.0 sec\n",
            "batch: 160 \t lr : 0.00025600000000000004 \t training loss : 0.15388278663158417 \t time taken : 40.9 sec\n",
            "batch: 170 \t lr : 0.00025600000000000004 \t training loss : 0.13614648580551147 \t time taken : 40.7 sec\n",
            "batch: 180 \t lr : 0.00025600000000000004 \t training loss : 0.13117557764053345 \t time taken : 42.8 sec\n",
            "batch: 190 \t lr : 0.00025600000000000004 \t training loss : 0.15314023196697235 \t time taken : 41.2 sec\n",
            "batch: 200 \t lr : 0.00025600000000000004 \t training loss : 0.1363571584224701 \t time taken : 39.8 sec\n",
            "batch: 210 \t lr : 0.00025600000000000004 \t training loss : 0.14142632484436035 \t time taken : 42.3 sec\n",
            "batch: 220 \t lr : 0.00025600000000000004 \t training loss : 0.1535344123840332 \t time taken : 42.1 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 21 \tval_loss 0.0056631379235874526 \tdistance 10.212272727272728\n",
            "Best loss: 0.0056631379235874526, epoch: 21\n",
            "training loss: 0.0056631379235874526, epoch: 21\n",
            "Epoch completed: 43.7 min\n",
            "LR =  0.00025600000000000004\n",
            "========================================\n",
            "Epoch:  22\n",
            "batch: 10 \t lr : 0.00025600000000000004 \t training loss : 0.12237156182527542 \t time taken : 45.1 sec\n",
            "batch: 20 \t lr : 0.00025600000000000004 \t training loss : 0.1200580820441246 \t time taken : 39.8 sec\n",
            "batch: 30 \t lr : 0.00025600000000000004 \t training loss : 0.14163000881671906 \t time taken : 41.7 sec\n",
            "batch: 40 \t lr : 0.00025600000000000004 \t training loss : 0.1099788248538971 \t time taken : 39.4 sec\n",
            "batch: 50 \t lr : 0.00025600000000000004 \t training loss : 0.1264573186635971 \t time taken : 38.8 sec\n",
            "batch: 60 \t lr : 0.00025600000000000004 \t training loss : 0.111223965883255 \t time taken : 42.8 sec\n",
            "batch: 70 \t lr : 0.00025600000000000004 \t training loss : 0.1100616455078125 \t time taken : 38.8 sec\n",
            "batch: 80 \t lr : 0.00025600000000000004 \t training loss : 0.11738892644643784 \t time taken : 40.6 sec\n",
            "batch: 90 \t lr : 0.00025600000000000004 \t training loss : 0.1402570605278015 \t time taken : 39.4 sec\n",
            "batch: 100 \t lr : 0.00025600000000000004 \t training loss : 0.11349184811115265 \t time taken : 40.9 sec\n",
            "batch: 110 \t lr : 0.00025600000000000004 \t training loss : 0.11300194263458252 \t time taken : 39.8 sec\n",
            "batch: 120 \t lr : 0.00025600000000000004 \t training loss : 0.0969599187374115 \t time taken : 41.8 sec\n",
            "batch: 130 \t lr : 0.00025600000000000004 \t training loss : 0.12008515000343323 \t time taken : 39.0 sec\n",
            "batch: 140 \t lr : 0.00025600000000000004 \t training loss : 0.12281975150108337 \t time taken : 40.4 sec\n",
            "batch: 150 \t lr : 0.00025600000000000004 \t training loss : 0.13379256427288055 \t time taken : 44.5 sec\n",
            "batch: 160 \t lr : 0.00025600000000000004 \t training loss : 0.1242918148636818 \t time taken : 40.4 sec\n",
            "batch: 170 \t lr : 0.00025600000000000004 \t training loss : 0.12372312694787979 \t time taken : 41.5 sec\n",
            "batch: 180 \t lr : 0.00025600000000000004 \t training loss : 0.1214090883731842 \t time taken : 41.0 sec\n",
            "batch: 190 \t lr : 0.00025600000000000004 \t training loss : 0.13299962878227234 \t time taken : 38.9 sec\n",
            "batch: 200 \t lr : 0.00025600000000000004 \t training loss : 0.1287773996591568 \t time taken : 40.4 sec\n",
            "batch: 210 \t lr : 0.00025600000000000004 \t training loss : 0.1419745832681656 \t time taken : 39.2 sec\n",
            "batch: 220 \t lr : 0.00025600000000000004 \t training loss : 0.13198786973953247 \t time taken : 43.7 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 22 \tval_loss 0.0057305093516003 \tdistance 10.065454545454546\n",
            "Epoch completed: 43.9 min\n",
            "LR =  0.00025600000000000004\n",
            "========================================\n",
            "Epoch:  23\n",
            "batch: 10 \t lr : 0.00025600000000000004 \t training loss : 0.12730403244495392 \t time taken : 45.3 sec\n",
            "batch: 20 \t lr : 0.00025600000000000004 \t training loss : 0.10626925528049469 \t time taken : 38.2 sec\n",
            "batch: 30 \t lr : 0.00025600000000000004 \t training loss : 0.12821048498153687 \t time taken : 39.6 sec\n",
            "batch: 40 \t lr : 0.00025600000000000004 \t training loss : 0.10832293331623077 \t time taken : 40.3 sec\n",
            "batch: 50 \t lr : 0.00025600000000000004 \t training loss : 0.12788838148117065 \t time taken : 41.4 sec\n",
            "batch: 60 \t lr : 0.00025600000000000004 \t training loss : 0.12219061702489853 \t time taken : 43.5 sec\n",
            "batch: 70 \t lr : 0.00025600000000000004 \t training loss : 0.11009524017572403 \t time taken : 40.9 sec\n",
            "batch: 80 \t lr : 0.00025600000000000004 \t training loss : 0.11907154321670532 \t time taken : 41.4 sec\n",
            "batch: 90 \t lr : 0.00025600000000000004 \t training loss : 0.13396422564983368 \t time taken : 40.0 sec\n",
            "batch: 100 \t lr : 0.00025600000000000004 \t training loss : 0.1004859060049057 \t time taken : 39.4 sec\n",
            "batch: 110 \t lr : 0.00025600000000000004 \t training loss : 0.11897683143615723 \t time taken : 40.1 sec\n",
            "batch: 120 \t lr : 0.00025600000000000004 \t training loss : 0.11998025327920914 \t time taken : 39.9 sec\n",
            "batch: 130 \t lr : 0.00025600000000000004 \t training loss : 0.11500859260559082 \t time taken : 40.4 sec\n",
            "batch: 140 \t lr : 0.00025600000000000004 \t training loss : 0.11540280282497406 \t time taken : 39.9 sec\n",
            "batch: 150 \t lr : 0.00025600000000000004 \t training loss : 0.11917105317115784 \t time taken : 40.0 sec\n",
            "batch: 160 \t lr : 0.00025600000000000004 \t training loss : 0.1207929402589798 \t time taken : 40.0 sec\n",
            "batch: 170 \t lr : 0.00025600000000000004 \t training loss : 0.13204091787338257 \t time taken : 39.2 sec\n",
            "batch: 180 \t lr : 0.00025600000000000004 \t training loss : 0.12075002491474152 \t time taken : 38.8 sec\n",
            "batch: 190 \t lr : 0.00025600000000000004 \t training loss : 0.15772473812103271 \t time taken : 44.6 sec\n",
            "batch: 200 \t lr : 0.00025600000000000004 \t training loss : 0.12612280249595642 \t time taken : 41.1 sec\n",
            "batch: 210 \t lr : 0.00025600000000000004 \t training loss : 0.1309504359960556 \t time taken : 41.6 sec\n",
            "batch: 220 \t lr : 0.00025600000000000004 \t training loss : 0.1331881880760193 \t time taken : 41.3 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 23 \tval_loss 0.00639098270372911 \tdistance 11.239090909090908\n",
            "Epoch completed: 44.1 min\n",
            "LR =  0.00025600000000000004\n",
            "========================================\n",
            "Epoch:  24\n",
            "batch: 10 \t lr : 0.00025600000000000004 \t training loss : 0.1647888720035553 \t time taken : 45.9 sec\n",
            "batch: 20 \t lr : 0.00025600000000000004 \t training loss : 0.12571027874946594 \t time taken : 39.3 sec\n",
            "batch: 30 \t lr : 0.00025600000000000004 \t training loss : 0.13503526151180267 \t time taken : 41.9 sec\n",
            "batch: 40 \t lr : 0.00025600000000000004 \t training loss : 0.11251382529735565 \t time taken : 41.9 sec\n",
            "batch: 50 \t lr : 0.00025600000000000004 \t training loss : 0.14985203742980957 \t time taken : 41.3 sec\n",
            "batch: 60 \t lr : 0.00025600000000000004 \t training loss : 0.13084767758846283 \t time taken : 40.4 sec\n",
            "batch: 70 \t lr : 0.00025600000000000004 \t training loss : 0.12598709762096405 \t time taken : 40.5 sec\n",
            "batch: 80 \t lr : 0.00025600000000000004 \t training loss : 0.13761131465435028 \t time taken : 41.7 sec\n",
            "batch: 90 \t lr : 0.00025600000000000004 \t training loss : 0.13814546167850494 \t time taken : 43.3 sec\n",
            "batch: 100 \t lr : 0.00025600000000000004 \t training loss : 0.1280021071434021 \t time taken : 38.3 sec\n",
            "batch: 110 \t lr : 0.00025600000000000004 \t training loss : 0.12363824248313904 \t time taken : 38.8 sec\n",
            "batch: 120 \t lr : 0.00025600000000000004 \t training loss : 0.1324971616268158 \t time taken : 40.4 sec\n",
            "batch: 130 \t lr : 0.00025600000000000004 \t training loss : 0.15466755628585815 \t time taken : 43.6 sec\n",
            "batch: 140 \t lr : 0.00025600000000000004 \t training loss : 0.13698819279670715 \t time taken : 39.8 sec\n",
            "batch: 150 \t lr : 0.00025600000000000004 \t training loss : 0.11866728961467743 \t time taken : 40.7 sec\n",
            "batch: 160 \t lr : 0.00025600000000000004 \t training loss : 0.12476726621389389 \t time taken : 38.4 sec\n",
            "batch: 170 \t lr : 0.00025600000000000004 \t training loss : 0.14632892608642578 \t time taken : 38.6 sec\n",
            "batch: 180 \t lr : 0.00025600000000000004 \t training loss : 0.12127611041069031 \t time taken : 41.2 sec\n",
            "batch: 190 \t lr : 0.00025600000000000004 \t training loss : 0.14739197492599487 \t time taken : 39.9 sec\n",
            "batch: 200 \t lr : 0.00025600000000000004 \t training loss : 0.16207078099250793 \t time taken : 39.4 sec\n",
            "batch: 210 \t lr : 0.00025600000000000004 \t training loss : 0.16517634689807892 \t time taken : 43.6 sec\n",
            "batch: 220 \t lr : 0.00025600000000000004 \t training loss : 0.1556001901626587 \t time taken : 41.4 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 24 \tval_loss 0.005891384793953462 \tdistance 10.235454545454546\n",
            "Epoch    24: reducing learning rate of group 0 to 2.0480e-04.\n",
            "Epoch completed: 43.9 min\n",
            "LR =  0.00020480000000000004\n",
            "========================================\n",
            "Epoch:  25\n",
            "batch: 10 \t lr : 0.00020480000000000004 \t training loss : 0.11118143796920776 \t time taken : 45.7 sec\n",
            "batch: 20 \t lr : 0.00020480000000000004 \t training loss : 0.12224328517913818 \t time taken : 40.8 sec\n",
            "batch: 30 \t lr : 0.00020480000000000004 \t training loss : 0.11832306534051895 \t time taken : 42.6 sec\n",
            "batch: 40 \t lr : 0.00020480000000000004 \t training loss : 0.13364577293395996 \t time taken : 38.5 sec\n",
            "batch: 50 \t lr : 0.00020480000000000004 \t training loss : 0.10421423614025116 \t time taken : 40.6 sec\n",
            "batch: 60 \t lr : 0.00020480000000000004 \t training loss : 0.11784103512763977 \t time taken : 39.0 sec\n",
            "batch: 70 \t lr : 0.00020480000000000004 \t training loss : 0.10513328015804291 \t time taken : 41.1 sec\n",
            "batch: 80 \t lr : 0.00020480000000000004 \t training loss : 0.09340934455394745 \t time taken : 38.5 sec\n",
            "batch: 90 \t lr : 0.00020480000000000004 \t training loss : 0.11568769812583923 \t time taken : 41.6 sec\n",
            "batch: 100 \t lr : 0.00020480000000000004 \t training loss : 0.1231786459684372 \t time taken : 41.5 sec\n",
            "batch: 110 \t lr : 0.00020480000000000004 \t training loss : 0.1302097737789154 \t time taken : 41.8 sec\n",
            "batch: 120 \t lr : 0.00020480000000000004 \t training loss : 0.12007344514131546 \t time taken : 39.2 sec\n",
            "batch: 130 \t lr : 0.00020480000000000004 \t training loss : 0.12538781762123108 \t time taken : 39.5 sec\n",
            "batch: 140 \t lr : 0.00020480000000000004 \t training loss : 0.10981938242912292 \t time taken : 39.1 sec\n",
            "batch: 150 \t lr : 0.00020480000000000004 \t training loss : 0.1235157921910286 \t time taken : 42.3 sec\n",
            "batch: 160 \t lr : 0.00020480000000000004 \t training loss : 0.12351082265377045 \t time taken : 41.1 sec\n",
            "batch: 170 \t lr : 0.00020480000000000004 \t training loss : 0.09198449552059174 \t time taken : 39.3 sec\n",
            "batch: 180 \t lr : 0.00020480000000000004 \t training loss : 0.10982179641723633 \t time taken : 40.5 sec\n",
            "batch: 190 \t lr : 0.00020480000000000004 \t training loss : 0.1267014741897583 \t time taken : 40.3 sec\n",
            "batch: 200 \t lr : 0.00020480000000000004 \t training loss : 0.12598033249378204 \t time taken : 40.5 sec\n",
            "batch: 210 \t lr : 0.00020480000000000004 \t training loss : 0.11624452471733093 \t time taken : 41.4 sec\n",
            "batch: 220 \t lr : 0.00020480000000000004 \t training loss : 0.0975043773651123 \t time taken : 39.6 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 25 \tval_loss 0.005683356943455609 \tdistance 9.61909090909091\n",
            "Epoch completed: 44.0 min\n",
            "LR =  0.00020480000000000004\n",
            "========================================\n",
            "Epoch:  26\n",
            "batch: 10 \t lr : 0.00020480000000000004 \t training loss : 0.1013464406132698 \t time taken : 43.2 sec\n",
            "batch: 20 \t lr : 0.00020480000000000004 \t training loss : 0.09352274239063263 \t time taken : 40.5 sec\n",
            "batch: 30 \t lr : 0.00020480000000000004 \t training loss : 0.0857030376791954 \t time taken : 39.9 sec\n",
            "batch: 40 \t lr : 0.00020480000000000004 \t training loss : 0.10113805532455444 \t time taken : 41.8 sec\n",
            "batch: 50 \t lr : 0.00020480000000000004 \t training loss : 0.10955246537923813 \t time taken : 41.4 sec\n",
            "batch: 60 \t lr : 0.00020480000000000004 \t training loss : 0.1312367171049118 \t time taken : 41.1 sec\n",
            "batch: 70 \t lr : 0.00020480000000000004 \t training loss : 0.10175321996212006 \t time taken : 39.2 sec\n",
            "batch: 80 \t lr : 0.00020480000000000004 \t training loss : 0.1336013376712799 \t time taken : 39.3 sec\n",
            "batch: 90 \t lr : 0.00020480000000000004 \t training loss : 0.12245477735996246 \t time taken : 39.4 sec\n",
            "batch: 100 \t lr : 0.00020480000000000004 \t training loss : 0.11959511041641235 \t time taken : 42.2 sec\n",
            "batch: 110 \t lr : 0.00020480000000000004 \t training loss : 0.1202550008893013 \t time taken : 40.6 sec\n",
            "batch: 120 \t lr : 0.00020480000000000004 \t training loss : 0.11220213770866394 \t time taken : 43.5 sec\n",
            "batch: 130 \t lr : 0.00020480000000000004 \t training loss : 0.11042793095111847 \t time taken : 41.7 sec\n",
            "batch: 140 \t lr : 0.00020480000000000004 \t training loss : 0.11680099368095398 \t time taken : 41.9 sec\n",
            "batch: 150 \t lr : 0.00020480000000000004 \t training loss : 0.10382767766714096 \t time taken : 40.8 sec\n",
            "batch: 160 \t lr : 0.00020480000000000004 \t training loss : 0.12647795677185059 \t time taken : 39.7 sec\n",
            "batch: 170 \t lr : 0.00020480000000000004 \t training loss : 0.12703630328178406 \t time taken : 41.7 sec\n",
            "batch: 180 \t lr : 0.00020480000000000004 \t training loss : 0.12025429308414459 \t time taken : 40.5 sec\n",
            "batch: 190 \t lr : 0.00020480000000000004 \t training loss : 0.11706391721963882 \t time taken : 38.7 sec\n",
            "batch: 200 \t lr : 0.00020480000000000004 \t training loss : 0.1138775646686554 \t time taken : 39.8 sec\n",
            "batch: 210 \t lr : 0.00020480000000000004 \t training loss : 0.10198640823364258 \t time taken : 38.5 sec\n",
            "batch: 220 \t lr : 0.00020480000000000004 \t training loss : 0.10518902540206909 \t time taken : 41.2 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 26 \tval_loss 0.0057919219542633404 \tdistance 9.651363636363635\n",
            "Epoch completed: 43.8 min\n",
            "LR =  0.00020480000000000004\n",
            "========================================\n",
            "Epoch:  27\n",
            "batch: 10 \t lr : 0.00020480000000000004 \t training loss : 0.0970543622970581 \t time taken : 46.7 sec\n",
            "batch: 20 \t lr : 0.00020480000000000004 \t training loss : 0.1183127760887146 \t time taken : 40.9 sec\n",
            "batch: 30 \t lr : 0.00020480000000000004 \t training loss : 0.10460689663887024 \t time taken : 41.6 sec\n",
            "batch: 40 \t lr : 0.00020480000000000004 \t training loss : 0.1128416657447815 \t time taken : 42.9 sec\n",
            "batch: 50 \t lr : 0.00020480000000000004 \t training loss : 0.10387593507766724 \t time taken : 40.9 sec\n",
            "batch: 60 \t lr : 0.00020480000000000004 \t training loss : 0.10235297679901123 \t time taken : 40.6 sec\n",
            "batch: 70 \t lr : 0.00020480000000000004 \t training loss : 0.10038881003856659 \t time taken : 41.5 sec\n",
            "batch: 80 \t lr : 0.00020480000000000004 \t training loss : 0.10523116588592529 \t time taken : 40.3 sec\n",
            "batch: 90 \t lr : 0.00020480000000000004 \t training loss : 0.09878009557723999 \t time taken : 39.8 sec\n",
            "batch: 100 \t lr : 0.00020480000000000004 \t training loss : 0.08504284173250198 \t time taken : 42.7 sec\n",
            "batch: 110 \t lr : 0.00020480000000000004 \t training loss : 0.08970260620117188 \t time taken : 38.9 sec\n",
            "batch: 120 \t lr : 0.00020480000000000004 \t training loss : 0.10650607198476791 \t time taken : 41.4 sec\n",
            "batch: 130 \t lr : 0.00020480000000000004 \t training loss : 0.10862362384796143 \t time taken : 40.3 sec\n",
            "batch: 140 \t lr : 0.00020480000000000004 \t training loss : 0.10504622757434845 \t time taken : 42.2 sec\n",
            "batch: 150 \t lr : 0.00020480000000000004 \t training loss : 0.08917073905467987 \t time taken : 39.8 sec\n",
            "batch: 160 \t lr : 0.00020480000000000004 \t training loss : 0.09663322567939758 \t time taken : 42.0 sec\n",
            "batch: 170 \t lr : 0.00020480000000000004 \t training loss : 0.1172676831483841 \t time taken : 40.0 sec\n",
            "batch: 180 \t lr : 0.00020480000000000004 \t training loss : 0.11382156610488892 \t time taken : 39.8 sec\n",
            "batch: 190 \t lr : 0.00020480000000000004 \t training loss : 0.11505171656608582 \t time taken : 40.6 sec\n",
            "batch: 200 \t lr : 0.00020480000000000004 \t training loss : 0.12648646533489227 \t time taken : 39.1 sec\n",
            "batch: 210 \t lr : 0.00020480000000000004 \t training loss : 0.11403709650039673 \t time taken : 41.1 sec\n",
            "batch: 220 \t lr : 0.00020480000000000004 \t training loss : 0.11023060977458954 \t time taken : 43.3 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 27 \tval_loss 0.005930610028180209 \tdistance 9.793636363636363\n",
            "Epoch    27: reducing learning rate of group 0 to 1.6384e-04.\n",
            "Epoch completed: 43.8 min\n",
            "LR =  0.00016384000000000006\n",
            "========================================\n",
            "Epoch:  28\n",
            "batch: 10 \t lr : 0.00016384000000000006 \t training loss : 0.10252086818218231 \t time taken : 48.5 sec\n",
            "batch: 20 \t lr : 0.00016384000000000006 \t training loss : 0.0945662409067154 \t time taken : 41.2 sec\n",
            "batch: 30 \t lr : 0.00016384000000000006 \t training loss : 0.08545331656932831 \t time taken : 40.6 sec\n",
            "batch: 40 \t lr : 0.00016384000000000006 \t training loss : 0.06925683468580246 \t time taken : 38.4 sec\n",
            "batch: 50 \t lr : 0.00016384000000000006 \t training loss : 0.10029499232769012 \t time taken : 39.1 sec\n",
            "batch: 60 \t lr : 0.00016384000000000006 \t training loss : 0.10418873280286789 \t time taken : 39.9 sec\n",
            "batch: 70 \t lr : 0.00016384000000000006 \t training loss : 0.10504895448684692 \t time taken : 41.6 sec\n",
            "batch: 80 \t lr : 0.00016384000000000006 \t training loss : 0.09107313305139542 \t time taken : 39.5 sec\n",
            "batch: 90 \t lr : 0.00016384000000000006 \t training loss : 0.08165958523750305 \t time taken : 42.5 sec\n",
            "batch: 100 \t lr : 0.00016384000000000006 \t training loss : 0.09827485680580139 \t time taken : 40.5 sec\n",
            "batch: 110 \t lr : 0.00016384000000000006 \t training loss : 0.09309253096580505 \t time taken : 40.1 sec\n",
            "batch: 120 \t lr : 0.00016384000000000006 \t training loss : 0.10000580549240112 \t time taken : 40.2 sec\n",
            "batch: 130 \t lr : 0.00016384000000000006 \t training loss : 0.08276151865720749 \t time taken : 40.5 sec\n",
            "batch: 140 \t lr : 0.00016384000000000006 \t training loss : 0.0967482328414917 \t time taken : 40.7 sec\n",
            "batch: 150 \t lr : 0.00016384000000000006 \t training loss : 0.1057349443435669 \t time taken : 42.6 sec\n",
            "batch: 160 \t lr : 0.00016384000000000006 \t training loss : 0.09210829436779022 \t time taken : 40.0 sec\n",
            "batch: 170 \t lr : 0.00016384000000000006 \t training loss : 0.10934768617153168 \t time taken : 41.6 sec\n",
            "batch: 180 \t lr : 0.00016384000000000006 \t training loss : 0.09575692564249039 \t time taken : 39.6 sec\n",
            "batch: 190 \t lr : 0.00016384000000000006 \t training loss : 0.1053389385342598 \t time taken : 39.9 sec\n",
            "batch: 200 \t lr : 0.00016384000000000006 \t training loss : 0.09270510822534561 \t time taken : 41.4 sec\n",
            "batch: 210 \t lr : 0.00016384000000000006 \t training loss : 0.08829744160175323 \t time taken : 39.0 sec\n",
            "batch: 220 \t lr : 0.00016384000000000006 \t training loss : 0.09234225004911423 \t time taken : 39.6 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 28 \tval_loss 0.005849922014908357 \tdistance 9.478181818181818\n",
            "Epoch completed: 44.1 min\n",
            "LR =  0.00016384000000000006\n",
            "========================================\n",
            "Epoch:  29\n",
            "batch: 10 \t lr : 0.00016384000000000006 \t training loss : 0.07723266631364822 \t time taken : 44.4 sec\n",
            "batch: 20 \t lr : 0.00016384000000000006 \t training loss : 0.08114960044622421 \t time taken : 39.4 sec\n",
            "batch: 30 \t lr : 0.00016384000000000006 \t training loss : 0.08212453126907349 \t time taken : 42.9 sec\n",
            "batch: 40 \t lr : 0.00016384000000000006 \t training loss : 0.0846368595957756 \t time taken : 41.1 sec\n",
            "batch: 50 \t lr : 0.00016384000000000006 \t training loss : 0.08972352743148804 \t time taken : 39.8 sec\n",
            "batch: 60 \t lr : 0.00016384000000000006 \t training loss : 0.08379463851451874 \t time taken : 41.3 sec\n",
            "batch: 70 \t lr : 0.00016384000000000006 \t training loss : 0.0855286493897438 \t time taken : 38.9 sec\n",
            "batch: 80 \t lr : 0.00016384000000000006 \t training loss : 0.07861796021461487 \t time taken : 40.3 sec\n",
            "batch: 90 \t lr : 0.00016384000000000006 \t training loss : 0.08922134339809418 \t time taken : 42.1 sec\n",
            "batch: 100 \t lr : 0.00016384000000000006 \t training loss : 0.09108336269855499 \t time taken : 43.5 sec\n",
            "batch: 110 \t lr : 0.00016384000000000006 \t training loss : 0.08081834763288498 \t time taken : 38.8 sec\n",
            "batch: 120 \t lr : 0.00016384000000000006 \t training loss : 0.08918459713459015 \t time taken : 43.9 sec\n",
            "batch: 130 \t lr : 0.00016384000000000006 \t training loss : 0.08292009681463242 \t time taken : 41.8 sec\n",
            "batch: 140 \t lr : 0.00016384000000000006 \t training loss : 0.07721573114395142 \t time taken : 40.7 sec\n",
            "batch: 150 \t lr : 0.00016384000000000006 \t training loss : 0.0906219482421875 \t time taken : 41.6 sec\n",
            "batch: 160 \t lr : 0.00016384000000000006 \t training loss : 0.08015939593315125 \t time taken : 41.3 sec\n",
            "batch: 170 \t lr : 0.00016384000000000006 \t training loss : 0.09269200265407562 \t time taken : 41.0 sec\n",
            "batch: 180 \t lr : 0.00016384000000000006 \t training loss : 0.06809067726135254 \t time taken : 38.4 sec\n",
            "batch: 190 \t lr : 0.00016384000000000006 \t training loss : 0.09976334124803543 \t time taken : 42.2 sec\n",
            "batch: 200 \t lr : 0.00016384000000000006 \t training loss : 0.0786578357219696 \t time taken : 39.1 sec\n",
            "batch: 210 \t lr : 0.00016384000000000006 \t training loss : 0.0902097299695015 \t time taken : 38.3 sec\n",
            "batch: 220 \t lr : 0.00016384000000000006 \t training loss : 0.09176843613386154 \t time taken : 39.4 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 29 \tval_loss 0.005903152579610998 \tdistance 9.462727272727273\n",
            "Epoch completed: 44.1 min\n",
            "LR =  0.00016384000000000006\n",
            "========================================\n",
            "Epoch:  30\n",
            "batch: 10 \t lr : 0.00016384000000000006 \t training loss : 0.06995046883821487 \t time taken : 45.2 sec\n",
            "batch: 20 \t lr : 0.00016384000000000006 \t training loss : 0.07527492940425873 \t time taken : 38.7 sec\n",
            "batch: 30 \t lr : 0.00016384000000000006 \t training loss : 0.07317885756492615 \t time taken : 40.9 sec\n",
            "batch: 40 \t lr : 0.00016384000000000006 \t training loss : 0.07939182221889496 \t time taken : 43.1 sec\n",
            "batch: 50 \t lr : 0.00016384000000000006 \t training loss : 0.08074381947517395 \t time taken : 41.6 sec\n",
            "batch: 60 \t lr : 0.00016384000000000006 \t training loss : 0.08195032179355621 \t time taken : 40.0 sec\n",
            "batch: 70 \t lr : 0.00016384000000000006 \t training loss : 0.08545102924108505 \t time taken : 38.9 sec\n",
            "batch: 80 \t lr : 0.00016384000000000006 \t training loss : 0.06298333406448364 \t time taken : 41.0 sec\n",
            "batch: 90 \t lr : 0.00016384000000000006 \t training loss : 0.08499379456043243 \t time taken : 43.6 sec\n",
            "batch: 100 \t lr : 0.00016384000000000006 \t training loss : 0.07799912989139557 \t time taken : 39.5 sec\n",
            "batch: 110 \t lr : 0.00016384000000000006 \t training loss : 0.06613510102033615 \t time taken : 42.7 sec\n",
            "batch: 120 \t lr : 0.00016384000000000006 \t training loss : 0.08656200766563416 \t time taken : 42.5 sec\n",
            "batch: 130 \t lr : 0.00016384000000000006 \t training loss : 0.0797959715127945 \t time taken : 39.3 sec\n",
            "batch: 140 \t lr : 0.00016384000000000006 \t training loss : 0.08300420641899109 \t time taken : 38.6 sec\n",
            "batch: 150 \t lr : 0.00016384000000000006 \t training loss : 0.08753965049982071 \t time taken : 39.9 sec\n",
            "batch: 160 \t lr : 0.00016384000000000006 \t training loss : 0.07677023112773895 \t time taken : 39.1 sec\n",
            "batch: 170 \t lr : 0.00016384000000000006 \t training loss : 0.07694657891988754 \t time taken : 39.8 sec\n",
            "batch: 180 \t lr : 0.00016384000000000006 \t training loss : 0.08535373955965042 \t time taken : 42.1 sec\n",
            "batch: 190 \t lr : 0.00016384000000000006 \t training loss : 0.08096179366111755 \t time taken : 39.5 sec\n",
            "batch: 200 \t lr : 0.00016384000000000006 \t training loss : 0.08469071984291077 \t time taken : 39.5 sec\n",
            "batch: 210 \t lr : 0.00016384000000000006 \t training loss : 0.081352099776268 \t time taken : 43.8 sec\n",
            "batch: 220 \t lr : 0.00016384000000000006 \t training loss : 0.07536844909191132 \t time taken : 39.3 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 30 \tval_loss 0.005933979912237687 \tdistance 9.383636363636363\n",
            "Epoch    30: reducing learning rate of group 0 to 1.3107e-04.\n",
            "Epoch completed: 43.8 min\n",
            "LR =  0.00013107200000000006\n",
            "========================================\n",
            "Epoch:  31\n",
            "batch: 10 \t lr : 0.00013107200000000006 \t training loss : 0.08597138524055481 \t time taken : 45.3 sec\n",
            "batch: 20 \t lr : 0.00013107200000000006 \t training loss : 0.07078850269317627 \t time taken : 43.0 sec\n",
            "batch: 30 \t lr : 0.00013107200000000006 \t training loss : 0.08262746781110764 \t time taken : 39.0 sec\n",
            "batch: 40 \t lr : 0.00013107200000000006 \t training loss : 0.07065161317586899 \t time taken : 40.1 sec\n",
            "batch: 50 \t lr : 0.00013107200000000006 \t training loss : 0.06860674172639847 \t time taken : 42.1 sec\n",
            "batch: 60 \t lr : 0.00013107200000000006 \t training loss : 0.07389900833368301 \t time taken : 42.6 sec\n",
            "batch: 70 \t lr : 0.00013107200000000006 \t training loss : 0.06678801774978638 \t time taken : 41.6 sec\n",
            "batch: 80 \t lr : 0.00013107200000000006 \t training loss : 0.06925426423549652 \t time taken : 41.0 sec\n",
            "batch: 90 \t lr : 0.00013107200000000006 \t training loss : 0.07203533500432968 \t time taken : 40.4 sec\n",
            "batch: 100 \t lr : 0.00013107200000000006 \t training loss : 0.07404391467571259 \t time taken : 41.1 sec\n",
            "batch: 110 \t lr : 0.00013107200000000006 \t training loss : 0.06442978978157043 \t time taken : 39.2 sec\n",
            "batch: 120 \t lr : 0.00013107200000000006 \t training loss : 0.07545369863510132 \t time taken : 42.7 sec\n",
            "batch: 130 \t lr : 0.00013107200000000006 \t training loss : 0.07480880618095398 \t time taken : 41.5 sec\n",
            "batch: 140 \t lr : 0.00013107200000000006 \t training loss : 0.07847783714532852 \t time taken : 39.2 sec\n",
            "batch: 150 \t lr : 0.00013107200000000006 \t training loss : 0.0777306854724884 \t time taken : 43.0 sec\n",
            "batch: 160 \t lr : 0.00013107200000000006 \t training loss : 0.09045597165822983 \t time taken : 40.0 sec\n",
            "batch: 170 \t lr : 0.00013107200000000006 \t training loss : 0.08067155629396439 \t time taken : 40.8 sec\n",
            "batch: 180 \t lr : 0.00013107200000000006 \t training loss : 0.07659156620502472 \t time taken : 41.0 sec\n",
            "batch: 190 \t lr : 0.00013107200000000006 \t training loss : 0.06900505721569061 \t time taken : 40.9 sec\n",
            "batch: 200 \t lr : 0.00013107200000000006 \t training loss : 0.07696431130170822 \t time taken : 40.5 sec\n",
            "batch: 210 \t lr : 0.00013107200000000006 \t training loss : 0.0866280272603035 \t time taken : 39.2 sec\n",
            "batch: 220 \t lr : 0.00013107200000000006 \t training loss : 0.0792798399925232 \t time taken : 40.2 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 31 \tval_loss 0.005893920483914288 \tdistance 9.347727272727273\n",
            "Epoch completed: 44.1 min\n",
            "LR =  0.00013107200000000006\n",
            "========================================\n",
            "Epoch:  32\n",
            "batch: 10 \t lr : 0.00013107200000000006 \t training loss : 0.05771338567137718 \t time taken : 47.6 sec\n",
            "batch: 20 \t lr : 0.00013107200000000006 \t training loss : 0.06858538091182709 \t time taken : 40.3 sec\n",
            "batch: 30 \t lr : 0.00013107200000000006 \t training loss : 0.06376264989376068 \t time taken : 39.6 sec\n",
            "batch: 40 \t lr : 0.00013107200000000006 \t training loss : 0.06463141739368439 \t time taken : 41.6 sec\n",
            "batch: 50 \t lr : 0.00013107200000000006 \t training loss : 0.05882902443408966 \t time taken : 41.8 sec\n",
            "batch: 60 \t lr : 0.00013107200000000006 \t training loss : 0.06112131103873253 \t time taken : 40.7 sec\n",
            "batch: 70 \t lr : 0.00013107200000000006 \t training loss : 0.07375244796276093 \t time taken : 41.7 sec\n",
            "batch: 80 \t lr : 0.00013107200000000006 \t training loss : 0.0701129138469696 \t time taken : 40.3 sec\n",
            "batch: 90 \t lr : 0.00013107200000000006 \t training loss : 0.06266601383686066 \t time taken : 40.6 sec\n",
            "batch: 100 \t lr : 0.00013107200000000006 \t training loss : 0.06519988179206848 \t time taken : 39.6 sec\n",
            "batch: 110 \t lr : 0.00013107200000000006 \t training loss : 0.05813729017972946 \t time taken : 39.4 sec\n",
            "batch: 120 \t lr : 0.00013107200000000006 \t training loss : 0.05355632305145264 \t time taken : 37.9 sec\n",
            "batch: 130 \t lr : 0.00013107200000000006 \t training loss : 0.06217033416032791 \t time taken : 41.0 sec\n",
            "batch: 140 \t lr : 0.00013107200000000006 \t training loss : 0.05225008726119995 \t time taken : 41.6 sec\n",
            "batch: 150 \t lr : 0.00013107200000000006 \t training loss : 0.065146304666996 \t time taken : 43.3 sec\n",
            "batch: 160 \t lr : 0.00013107200000000006 \t training loss : 0.07755095511674881 \t time taken : 42.6 sec\n",
            "batch: 170 \t lr : 0.00013107200000000006 \t training loss : 0.0886310413479805 \t time taken : 39.6 sec\n",
            "batch: 180 \t lr : 0.00013107200000000006 \t training loss : 0.06335124373435974 \t time taken : 40.8 sec\n",
            "batch: 190 \t lr : 0.00013107200000000006 \t training loss : 0.06658507138490677 \t time taken : 40.0 sec\n",
            "batch: 200 \t lr : 0.00013107200000000006 \t training loss : 0.08332211524248123 \t time taken : 41.7 sec\n",
            "batch: 210 \t lr : 0.00013107200000000006 \t training loss : 0.07484818249940872 \t time taken : 39.5 sec\n",
            "batch: 220 \t lr : 0.00013107200000000006 \t training loss : 0.07235151529312134 \t time taken : 40.2 sec\n",
            "validating – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –\n",
            "Epoch 32 \tval_loss 0.005968011617660522 \tdistance 9.315909090909091\n",
            "Epoch completed: 43.9 min\n",
            "LR =  0.00013107200000000006\n",
            "========================================\n",
            "Epoch:  33\n",
            "batch: 10 \t lr : 0.00013107200000000006 \t training loss : 0.06000147759914398 \t time taken : 47.3 sec\n",
            "batch: 20 \t lr : 0.00013107200000000006 \t training loss : 0.05861830711364746 \t time taken : 40.9 sec\n",
            "batch: 30 \t lr : 0.00013107200000000006 \t training loss : 0.06925968825817108 \t time taken : 40.4 sec\n",
            "batch: 40 \t lr : 0.00013107200000000006 \t training loss : 0.06243292987346649 \t time taken : 39.1 sec\n",
            "batch: 50 \t lr : 0.00013107200000000006 \t training loss : 0.06670714914798737 \t time taken : 38.5 sec\n",
            "batch: 60 \t lr : 0.00013107200000000006 \t training loss : 0.06515040993690491 \t time taken : 38.7 sec\n",
            "batch: 70 \t lr : 0.00013107200000000006 \t training loss : 0.060383398085832596 \t time taken : 45.1 sec\n",
            "batch: 80 \t lr : 0.00013107200000000006 \t training loss : 0.07454314827919006 \t time taken : 42.8 sec\n",
            "batch: 90 \t lr : 0.00013107200000000006 \t training loss : 0.06347791105508804 \t time taken : 40.8 sec\n",
            "batch: 100 \t lr : 0.00013107200000000006 \t training loss : 0.07191458344459534 \t time taken : 41.6 sec\n",
            "batch: 110 \t lr : 0.00013107200000000006 \t training loss : 0.06024784594774246 \t time taken : 41.3 sec\n",
            "batch: 120 \t lr : 0.00013107200000000006 \t training loss : 0.0674191415309906 \t time taken : 39.5 sec\n",
            "batch: 130 \t lr : 0.00013107200000000006 \t training loss : 0.06732845306396484 \t time taken : 40.4 sec\n",
            "batch: 140 \t lr : 0.00013107200000000006 \t training loss : 0.07963140308856964 \t time taken : 42.2 sec\n",
            "batch: 150 \t lr : 0.00013107200000000006 \t training loss : 0.06896872073411942 \t time taken : 41.3 sec\n",
            "batch: 160 \t lr : 0.00013107200000000006 \t training loss : 0.06926019489765167 \t time taken : 39.4 sec\n",
            "batch: 170 \t lr : 0.00013107200000000006 \t training loss : 0.06501346826553345 \t time taken : 40.0 sec\n",
            "batch: 180 \t lr : 0.00013107200000000006 \t training loss : 0.07386036217212677 \t time taken : 42.1 sec\n",
            "batch: 190 \t lr : 0.00013107200000000006 \t training loss : 0.058493658900260925 \t time taken : 44.5 sec\n",
            "batch: 200 \t lr : 0.00013107200000000006 \t training loss : 0.06524476408958435 \t time taken : 41.0 sec\n",
            "batch: 210 \t lr : 0.00013107200000000006 \t training loss : 0.07526230067014694 \t time taken : 41.6 sec\n",
            "batch: 220 \t lr : 0.00013107200000000006 \t training loss : 0.07354666292667389 \t time taken : 41.9 sec\n",
            "validating – – – – – – – – – –"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "epj4rwYn1hBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMf9iggmklkz"
      },
      "source": [
        "mask = np.where(val_loss_array != 0)\n",
        "epoch_array = epoch_array[mask]\n",
        "train_loss_array = train_loss_array[mask]\n",
        "val_loss_array = val_loss_array[mask]\n",
        "\n",
        "plt.figure(dpi=300)\n",
        "plt.yscale('log')\n",
        "plt.plot(epoch_array, train_loss_array, color = 'r', label='Training Loss')\n",
        "plt.plot(epoch_array, val_loss_array, color = 'g', label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig(model_path+'.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXaTRHK_WCYO"
      },
      "source": [
        "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQVV3_LwRoAl"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "cell inserted to break code flow\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvD2JK1ifND_"
      },
      "source": [
        "# 5 Test Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.0 some extra code for simultaneous testing from different account"
      ],
      "metadata": {
        "id": "PcC2d2KAkog2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sStalr5F_DU"
      },
      "source": [
        "#### delete next two cells later"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skpJVX87UegK"
      },
      "source": [
        "#test prereqs\n",
        "# Google drive setup\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "\n",
        "# ctcdecode\n",
        "print('\\n-----------Downloading ctcdecode----------\\n')\n",
        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "!pip install wget\n",
        "%cd ctcdecode\n",
        "\n",
        "!pip install .\n",
        "%cd ..\n",
        "print('\\n-----------Done----------\\n')\n",
        "\n",
        "#Levenshtein\n",
        "print('\\n-----------Downloading Levenshtein----------\\n')\n",
        "!pip install python-Levenshtein\n",
        "print('\\n-----------Done----------\\n')\n",
        "\n",
        "\n",
        "\n",
        "#imports\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import Levenshtein\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pdb\n",
        "import gc\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# Define dataset class\n",
        "class MyDataSet(Dataset):\n",
        "  # load the dataset\n",
        "  def __init__(self, x, y):\n",
        "    self.X = x\n",
        "    self.Y = y\n",
        "\n",
        "  # get number of items/rows in dataset\n",
        "  def __len__(self):\n",
        "    return len(self.Y)\n",
        "\n",
        "  # get row item at some index\n",
        "  def __getitem__(self, index):\n",
        "    x = torch.FloatTensor(self.X[index])\n",
        "    y = torch.LongTensor(self.Y[index])\n",
        "    return x, y\n",
        "\n",
        "  def collate_fn(batch):\n",
        "    batch_x = [x for x,y in batch]\n",
        "    batch_y = [y for x,y in batch]\n",
        "    lengths_x = torch.as_tensor([len(x) for x in batch_x])\n",
        "    lengths_y = torch.as_tensor([len(y) for y in batch_y])\n",
        "    padded_batch_x = pad_sequence(batch_x, batch_first=True)\n",
        "    padded_batch_y = pad_sequence(batch_y, batch_first=True)\n",
        "    return padded_batch_x, padded_batch_y, lengths_x, lengths_y\n",
        "\n",
        "    \n",
        "# Define dataset class\n",
        "class TestDataSet(Dataset):\n",
        "  # load the dataset\n",
        "  # TODO: replace x and y with dataset path and load data from here -> more efficient\n",
        "  def __init__(self, x):\n",
        "    self.X = x\n",
        "\n",
        "  # get number of items/rows in dataset\n",
        "  def __len__(self):\n",
        "    return len(self.X) \n",
        "\n",
        "  # get row item at some index\n",
        "  def __getitem__(self, index):\n",
        "    x = torch.FloatTensor(self.X[index])\n",
        "    return x\n",
        "\n",
        "  def collate_fn(batch_x):\n",
        "    lengths_x = torch.as_tensor[len(x) for x in batch_x])\n",
        "    padded_batch_x = pad_sequence(batch_x, batch_first=True, padding_value=0)\n",
        "    return padded_batch_x, lengths_x\n",
        "\n",
        "\n",
        "\n",
        "#data\n",
        "# train = torch.load('/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/train_dataset.pt')\n",
        "# train_labels = torch.load(\"/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/train_labels_dataset.pt\")\n",
        "# dev = torch.load(\"/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/dev_dataset.pt\")\n",
        "# dev_labels = torch.load(\"/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/dev_labels_dataset.pt\")\n",
        "# test = torch.load(\"/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/test_dataset.pt\")\n",
        "\n",
        "#loaders\n",
        "train_loader = torch.load(\"/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/train_loader.pt\")\n",
        "dev_loader = torch.load(\"/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/dev_loader.pt\")\n",
        "test_loader = torch.load(\"/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_datasets/test_loader.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvu8hYNmR4Qr"
      },
      "source": [
        "# Check if cuda is available and set device\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "NUM_WORKERS = 8 if cuda else 0\n",
        "\n",
        "print(\"Cuda = \", str(cuda), \" with num_workers = \", str(NUM_WORKERS),  \" system version = \", sys.version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmJdGkXMR4Qs"
      },
      "source": [
        "# TODO: Create model    \n",
        "\n",
        "class CBR(nn.Sequential):\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super().__init__(\n",
        "        nn.Conv1d(input_size, output_size, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm1d(output_size),\n",
        "        nn.ReLU(output_size)\n",
        "    )\n",
        "  \n",
        "\n",
        "class ConvBlock(nn.Sequential):\n",
        "  def __init__(self, input_size, narrow, wide):\n",
        "    super().__init__(\n",
        "        CBR(input_size, narrow),\n",
        "        CBR(narrow, narrow),\n",
        "        CBR(narrow, wide),\n",
        "        CBR(wide, wide),\n",
        "        CBR(wide, wide)\n",
        "    )\n",
        "\n",
        "\n",
        "class LSTMModel(nn.Sequential):\n",
        "  def __init__(self, embedding_size, lstm_output_size, num_lstm_layers, bi=False):\n",
        "    super().__init__(\n",
        "        nn.LSTM(embedding_size, lstm_output_size, num_lstm_layers, bidirectional=bi, batch_first=True, dropout=0.2)\n",
        "    )\n",
        "\n",
        "\n",
        "class LinearBlock(nn.Sequential):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super().__init__(\n",
        "        nn.Linear(input_size, hidden_size),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(hidden_size, output_size)\n",
        "    )\n",
        "\n",
        "\n",
        "class Seq2SeqModel(nn.Module):\n",
        "  def __init__(self, input_size, conv_hiddens, lstm_units, lstm_layers, bi, linear_hidden, output_size):\n",
        "    directions = 2 if bi else 1\n",
        "    linear_input = lstm_units * directions\n",
        "    super().__init__()\n",
        "    self.conv = ConvBlock(40, conv_hiddens[0], conv_hiddens[1])\n",
        "    self.lstm = LSTMModel(conv_hiddens[1], lstm_units, num_lstm_layers=lstm_layers, bi=bi)    \n",
        "    self.linear = LinearBlock(linear_input, linear_hidden, output_size)\n",
        "\n",
        "  def forward(self, x, xLens): # x dim (B, T_in, C_in=40)\n",
        "    x_cnn_input = x.permute(0, 2, 1) # (B, C_in, T_in)\n",
        "    x_post_cnn = self.conv(x_cnn_input) # (B, C_out, T_out)\n",
        "    x_rnn_in = x_post_cnn.permute(2, 0, 1) # (T, B, C_out)\n",
        "    x_packed = pack_padded_sequence(x_rnn_in, xLens, enforce_sorted=False)\n",
        "    out_packed, hidden = self.lstm(x_packed)\n",
        "    out, out_lens = pad_packed_sequence(out_packed, batch_first=True) # (B, T, C)\n",
        "    \n",
        "    # Log softmax after output layer is required since nn.CTCLoss expect log prob\n",
        "    out_prob = self.linear(out).log_softmax(2) # (B, T, Classes=47)\n",
        "    \n",
        "    # Permute to fit for input format of CTCLoss\n",
        "    out_prob = out_prob.permute(1, 0, 2) #torch.transpose(out_prob, 0, 1) # (T, B, C)\n",
        "    \n",
        "    # TODO: calculate new xLens\n",
        "    return out_prob, xLens\n",
        "\n",
        "\n",
        "  # def forward(self, x_padded, x_lens):\n",
        "  #   x_padded = self.conv(x_padded.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "  #   x_packed = pack_padded_sequence(x_padded, x_lens, batch_first=True, enforce_sorted=False)\n",
        "  #   output_packed, _ = self.lstm(x_packed)\n",
        "  #   output_padded, output_lens = pad_packed_sequence(output_packed, batch_first=True)\n",
        "  #   # Log softmax after output layer is required since`nn.CTCLoss` expects log probabilities.\n",
        "  #   output = self.linear(output_padded).log_softmax(2)\n",
        "  #   return output, output_lens\n",
        "\n",
        "\n",
        "  # def forward(self, x, len_x):\n",
        "  #   # x is already padded, prepare for cnn\n",
        "  #   x = x.permute(0,2,1) #batch_size * channels * \n",
        "  #   x = self.conv(x)\n",
        "  #   # pack x and prepare for lstm\n",
        "  #   x = x.permute(2,0,1) #\n",
        "  #   x = pack_padded_sequence(x, len_x, batch_first=True, enforce_sorted=False)\n",
        "  #   out, len_out_packed = self.lstm(x)\n",
        "  #   # pad the packed output\n",
        "  #   out, len_out = pad_packed_sequence(out, batch_first=True)\n",
        "  #   # linear layers\n",
        "  #   out = self.linear(out)\n",
        "  #   # log loss as required by CTC loss function\n",
        "  #   out = out.log_softmax(2)\n",
        "  #   return out, len_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTiSWN1rR4Qs"
      },
      "source": [
        "# create model\n",
        "input_size = 40\n",
        "conv_hiddens = [128, 256]\n",
        "lstm_units = 512\n",
        "lstm_layers = 4\n",
        "bidirectional = True\n",
        "linear_hidden = 512\n",
        "output_size = 42\n",
        "\n",
        "model_path = '/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_models/test_best_model_10_lronp.pt'\n",
        "\n",
        "\n",
        "model = Seq2SeqModel(input_size, conv_hiddens, lstm_units,  lstm_layers, bidirectional, linear_hidden, output_size)\n",
        "try:\n",
        "  model.load_state_dict(torch.load(model_path))\n",
        "  print('model keys matched\\n\\n')\n",
        "except:\n",
        "  print('didnt load any model')\n",
        "model = model.to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrXCXCZ3R4Qs"
      },
      "source": [
        "# Hyperparams\n",
        "LEARNING_RATE = 2e-3\n",
        "WEIGHT_DECAY = 5e-5\n",
        "\n",
        "\n",
        "criterion = nn.CTCLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.8, patience=2, verbose=True)\n",
        "\n",
        "# ReduceLRONP\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhr78fQER4Qs"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"data/HW3P2_Data\")\n",
        "sys.path.append('/content/gdrive/MyDrive/IDL-Kaggle/hw3')\n",
        "\n",
        "from phoneme_list import PHONEME_MAP, PHONEME_LIST\n",
        "print(PHONEME_LIST)\n",
        "print(PHONEME_MAP)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZl99aN9UqY2"
      },
      "source": [
        "def idx2phonemes(target):\n",
        "    return \"\".join([PHONEME_MAP[x] for x in target])\n",
        "\n",
        "def calculateLevScore(w1, w2):\n",
        "    return Levenshtein.distance(w1.replace(\" \", \"\"), w2.replace(\" \", \"\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0RYLbkjR4Qt"
      },
      "source": [
        "from ctcdecode import CTCBeamDecoder\n",
        "\n",
        "\n",
        "class Decoder(CTCBeamDecoder):\n",
        "  def __init__(self, beam_width=100):\n",
        "    self.decoder = CTCBeamDecoder(labels=PHONEME_MAP, beam_width=beam_width, log_probs_input=True)\n",
        "\n",
        "  def decode(self, output, len_x): \n",
        "    # print('output unprocessed', output.shape)\n",
        "    output = torch.transpose(output, 0, 1)\n",
        "    beam_results, beam_scores, timesteps, len_out = self.decoder.decode(output, len_x)\n",
        "    # print('beam_results', beam_results.shape)\n",
        "\n",
        "    #TCLO\n",
        "    decodedListShort = []\n",
        "    for b in range(beam_results.size(0)):\n",
        "        currDecode = \"\"\n",
        "        if len_out[b][0] != 0:\n",
        "            currDecodeShort = \"\".join([PHONEME_MAP[i] for i in beam_results[b, 0, :len_out[b][0]]])\n",
        "        decodedListShort.append(currDecodeShort)\n",
        "    return decodedListShort\n",
        "    \n",
        "    def get_edit_dist(self, output, output_lens, target, target_lens):\n",
        "      output, target = output.cpu(), target.cpu()\n",
        "      phonome_preds = self.decoder.decode(output, output_lens)\n",
        "      phonomes = self.decoder.convert_to_strings(target, target_lens)\n",
        "      edit_dist = np.sum([self.decoder.Lev_dist(phonome_pred, phonome) for (phonome_pred, phonome) in zip(phonome_preds, phonomes)])\n",
        "      return edit_dist\n",
        "  \n",
        "# TODO: Initialize decoder here\n",
        "decoder = Decoder()\n",
        "# In CTCBeamDecoder beam_width=1 (greedy search); beam_width>1 (beam search)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1MvZuK2b4TO"
      },
      "source": [
        "## 5.1 Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3HvaXAC08IP"
      },
      "source": [
        "#tclo\n",
        "predictions = []\n",
        "\n",
        "\n",
        "def test_model(model, data_loader):\n",
        "    start = time.time()\n",
        "    model.eval()\n",
        "    sample_num = 0\n",
        "    with torch.no_grad():\n",
        "      print('testing starting ...')\n",
        "      for batch, (x, len_x) in enumerate(test_loader):\n",
        "        x = x.to(device)\n",
        "        out, len_out = model(x, len_x)\n",
        "        preds = decoder.decode(out, len_x)\n",
        "        predictions.extend(preds)\n",
        "        sample_num += len(len_x)\n",
        "        print(f'saved batch:\\t {batch}/{len(test_loader)}\\t time elapsed = {(time.time()-start)//6/10} min')\n",
        "    \n",
        "    print('Testing done, predictions updated')\n",
        "    indices = np.arange(len(predictions))\n",
        "    d = {'id': indices, 'label': np.array(predictions)}\n",
        "    df = pd.DataFrame(data=d)\n",
        "    df.to_csv('/content/gdrive/MyDrive/IDL-Kaggle/hw3/predictions.csv', header=True, index=False)\n",
        "    print('predictions complete')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_gyFuOQkkZ5"
      },
      "source": [
        "predictions = np.array(predictions[2561:])\n",
        "np.save('predictions.npy', predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoBNSkLh4D2n"
      },
      "source": [
        "print(len(test_loader))\n",
        "model.load_state_dict(torch.load('/content/gdrive/MyDrive/IDL-Kaggle/hw3/saved_models/test_best_model.pt'))\n",
        "test_model(model, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4Omhr1kzwsZ"
      },
      "source": [
        "indices = np.arange(len(predictions))\n",
        "d = {'id': indices, 'label': np.array(predictions)}\n",
        "df = pd.DataFrame(data=d)\n",
        "df.to_csv('/content/gdrive/MyDrive/IDL-Kaggle/hw3/predictions.csv', header=True, index=False)\n",
        "print('predictions complete')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMleXECd4pih"
      },
      "source": [
        "#tclo\n",
        "predictions = []\n",
        "\n",
        "def test_model(model, data_loader):\n",
        "    start = time.time()\n",
        "    model.eval()\n",
        "    sample_num = 0\n",
        "    with torch.no_grad():\n",
        "      print('testing starting ...')\n",
        "      for batch, (x, len_x) in enumerate(test_loader):\n",
        "        x = x.to(device)\n",
        "        out, len_out = model(x, len_x)\n",
        "        preds = decoder.decode(out, len_x)\n",
        "        predictions.extend(preds)\n",
        "        sample_num += len(len_x)\n",
        "        print(f'saved batch:\\t {batch}/{len(test_loader)}\\t time elapsed = {(time.time()-start)//6/10} min')\n",
        "    \n",
        "    print('Testing done, predictions updated')\n",
        "    indices = np.arange(len(predictions))\n",
        "    d = {'id': indices, 'label': np.array(predictions)}\n",
        "    df = pd.DataFrame(data=d)\n",
        "    df.to_csv('/content/gdrive/MyDrive/IDL-Kaggle/hw3/predictions.csv', header=True, index=False)\n",
        "    print('predictions complete')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I9VwH_ppG3b"
      },
      "source": [
        "## 5.2 Save Predictions to csv File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShgVcbGtiSNQ"
      },
      "source": [
        "## 5.3 Submit Predictions"
      ]
    }
  ]
}